TAMPERE UNIVERSITY OF TECHNOLOGY
Department of Computer Sciences

HENRI BRAGGE
WEB MANAGEMENT SOFTWARE FOR AN SHDSL TERMINAL DEVICE

Topic approved at the department council meeting on February 12, 2007.
Examiner: Professor Pekka Loula (TUT)

Preamble

I completed my master's thesis while working as a software developer at Design Combus Oy in Tampere. The thesis deals with the creation of a graphical user interface for a company's product, and its purpose is to facilitate the use of the product.

Tampere, April 1, 2007
Henri Bragge

Abstract

TAMPERE UNIVERSITY OF TECHNOLOGY
Department of Information Technology
Information Technology (Pori)

BRAGGE, HENRI: Web management software for an SHDSL terminal device
Master of Science Thesis, 82 pages, 18 enclosure pages
Examiner: Professor Pekka Loula
Funding: Design Combus Ltd
April 2007
Keywords: HTTP, server, SHDSL, web
UDK: 004.455.2

Network terminal devices, like routers, switches and modems are devices that interconnect computer networks. Their main features include controlling and adapting the network traffic and communication to other network devices. Terminal devices often require management from the owner or the administrator of the device. Management includes tasks like configuring the initial settings, configuring settings to adapt to changes in the environment or upgrading the device’s software. Terminal devices offer different management possibilities, depending on the application environment of the device. Most devices offer terminal or Web-based management applications or both of them.

This thesis describes the developing of Web-based management software for an SHDSL (Symmetric High-speed Digital Subscriber Line) terminal device. The main emphasis is on the theory behind the problems confronted, the most important of which are the operation of HTTP server software and the software and hardware environments of the target device. The thesis also describes the structure, operation and further concerns of the management software. Before the implementation it is essential to study Web server theory and the features of the target environment and to consider frameworks, management software and Web servers created before. Thus it is possible to utilize existing knowledge and to ensure that the implemented application is modern and appropriate.

The implementation phase was challenging and demanded careful planning because of the requirements for versatile and general-purpose software and the limitations of the target environment. The final product was preceded by numerous different prototypes, from which the application slowly took its final shape.

Abbreviations

ADSL	Asymmetric Digital Subscriber Line
AMPED	Asymmetric Multi-process Event-driven
API	Application Programming Interface
ASP	Active Server Pages
BSD	Berkeley Software Distribution
CGI	Common Gateway Interface
CLI	Command Line Interface
CPU	Central Processing Unit
CSS	Cascading Style Sheets
CVS	Concurrent Versions System
DDR	Double Data Rate
DES	Data Encryption Standard
DMA	Direct Memory Access
DNS	Domain Name System
DSLAM	Digital Subscriber Line Access Multiplexer
DoS	Denial-of-Service
EEPROM	Electrically Erasable Programmable Read Only Memory
FIFO	First In, First Out
FPGA	Field-programmable gate Array
GPL	GNU Public License
HTML	Hypertext Markup Language
HTTP	Hypertext Transfer Protocol
I2C	Inter-Integrated Circuit
I/O	Input/Output
IETF	Internet Engineering Task Force
IP	Internet Protocol
IPC	Inter-process Communication
ISAPI	Internet Server Application Programming Interface
J2EE	Java 2 Platform Enterprise Edition
JTAG	Joint Test Action Group
LED	Light-emitting Diode
MD5	Message-Digest algorithm 5
MFC	Microsoft Foundation Classes
MII	Media-independent Interface
MIPS	Microprocessor without Interlocked Pipeline Stages
MIT	Massachusetts Institute of Technology
MVC	Model-View-Control
PCI	Peripheral Component Interconnect
PHP	PHP: Hypertext Preprocessor
PKI	Public Key Infrastructure
RAM	Random Access Memory
RC2	Rivest Cipher 2
RC4	Rivest Cipher 4
RFC	Request for Comments
RJ45	Registered Jack 45
RJ6/6	Registered Jack 6/6
ROM	Read-only Memory
RSA	Rivest, Shamir, Adleman algorithm
RSS	Really Simple Syndication
SAPI	Server Application Programming Interface
SHA	Secure Hash Algorithm
SHDSL	Symmetric High-speed Digital Subscriber Line
SNMP	Simple Network Management Protocol
SPED	Single-process Event-driven
SPI	Serial Peripheral Interface
SQL	Structured Query Language
SSL	Secure Sockets Layer
STDIN	Standard Input
STL	Standard Template Library
SVN	Subversion
TCP	Transfer Control Protocol
TLS	Transport Layer Security
UART	Universal Asynchronous Receiver/Transmitter
UDP	User Datagram Protocol
URI	Uniform Resource Identifier
URL	Uniform Resource Locator
VLAN	Virtual Local Area Network
W3C	World Wide Web Consortium
WWW	World Wide Web
XSL	Extensible Stylesheet Language

1 Introduction

The management capability of network terminal equipment is an essential part of the device because the devices rarely operate with the default settings pre-stored in them. Terminal equipment designed for businesses, such as the SHDSL terminal equipment targeted in this project, differs from ADSL (Asymmetric Digital Subscriber Line) modems designed for private customers, for example, in that the target group responsible for their management can generally be assumed to be more technically savvy than a private customer. In this case, adding so-called wizard features and hiding extra technical information can be given less attention. However, this does not diminish the importance of good usability. On the contrary, usability problems may bother an expert in the field more than a casual user who only occasionally handles the device's features. In addition, learnability and memorability are important features regardless of the user's level.

In addition to usability, two important aspects must be considered when designing a web application for an embedded device: the criticality of the target environment and the flexible design of the application. The memory capacity and computing power of an embedded device are limited, so the program must be lightweight and also take into account other, perhaps more important, processes on the device. It must also not stop functioning in the event of an error, and must under no circumstances endanger the device's functionality. The program should be flexible, as it is likely that a similar application will also be needed in future product projects when they become relevant.

This work explores the aforementioned design principles and the theory behind them, based on which the web management application presented in the work is created. The application's ultimate goal is to be user-friendly, reusable, and well-integrated into its target environment. The application will not be the sole means of managing the Iris 440 SHDSL terminal; instead, it will operate alongside the previously implemented command-line-based CLI (Command Line Interface) management application. Additionally, the device's features can be viewed using an SNMP (Simple Network Management Protocol) manager. The web management application is intended to function alongside these, providing the user with a graphical and easy-to-use alternative for managing the device.

Chapter two deals with the theory related to HTTP servers: the protocol itself, inter-process communication, dynamic content creation, process models, security, and application architectures. The chapter covers the background theory of server applications required for the project. The theory discussed in the chapter is important in order to understand the operating principles of the application being implemented and its communication methods with the user and the rest of the environment.

Chapter three examines the target environment of the project, which is the Iris 440 SHDSL terminal device designed for business use. In addition to hardware and performance aspects, the focus is on the device's operating system and application environment, as well as the existing CLI management application that runs alongside the web management application. When implementing a new application, it is advisable to examine the weaknesses and strengths of the previous solution and, where possible, reuse the program code created for it. Regarding the application development environment, we will familiarize ourselves with the compilation tools and version control methods used.

Chapter four presents the server mapping conducted during the preliminary research phase of the project. It examines a set of HTTP server applications that could be suitable for the task of the server acting as the core of the management application. The server applications are classified according to certain classification criteria, and the most suitable option for this purpose is selected from among them. The classification criteria are based on the understanding developed in the preceding chapters regarding the requirements of a server application in terms of its features and functionality.

Chapter five discusses the design and implementation of the administrative application. The main focus of the chapter is on the design of the application's views and internal functionality. The implementation of the application is not covered step-by-step, but rather focuses on examining the final result within a suitable scope. The chapter progresses in a sequence corresponding to the workflow, first addressing the design of the user interface and its functions, then the class architecture of the web application and the internal operation of the classes, and finally, the practical formation of the server response.

Numbers six and seven involve evaluating the final result in terms of its suitability for its end use and its potential for further development. In particular, the transferability of the final result is considered with regard to possible future product projects.

2 HTTP server application

An HTTP server application is an application that listens on the server's HTTP port (usually port 80) and responds to incoming requests. Before sending a response, several steps occur after receiving a request, including parsing the request, processing it, retrieving the necessary data from the server, and forming the response. This chapter discusses these steps and other aspects related to the operation of a server application. The chapter first describes the HTTP protocol used for communication and then the server's operation from the operating system's perspective. In addition, it addresses the security of the server application and, finally, web application software architectures.

2.1 World Wide Web

The World Wide Web saw the light of day in the spring of 1989 when Tim Berners-Lee published his proposal for a new way to present and link information. This started a new era in digital data transmission, which, through uniform resource identification and sharing as well as hypertext, enabled the sharing of audio, images, text, and other information with graphical browsers. The diversity of transmittable information quickly created a need for dynamic information creation and processing as well. Today, it enables interactive applications on WWW servers, the use of which via a browser over the internet is as easy and fast as with traditional applications locally.

2.1.1 HTTP Protocol

HTTP is a data transfer protocol used on the WWW, which allows a client and a server to negotiate with each other. HTTP was developed by the W3C (World Wide Web Consortium) and the IETF (Internet Engineering Task Force), and the most common version currently used is HTTP/1.1, which is defined by RFC 2616 [Ber99]. Different versions are backwards compatible, and version 1.1 includes improvements that streamline HTTP traffic and provide better control over the appearance of WWW pages [Pus01, p. 127]. HTTP operates on the application layer, and the TCP protocol is generally used below it, providing HTTP with reliable and connection-oriented data transfer. This subsection is based on sources [Ber99] and [Pel98, pp. 19-21].

Request

HTTP defines a set of methods and corresponding response codes that are used for communication between the user and the server. Through methods, the user expresses to the server what they want to do, and usually the URI of the target is also provided. The response sent by the server always includes a response code, which indicates the status of the response.

One of the most common HTTP methods is GET, which is used to retrieve data from a specified URI. In addition to the URI, the request usually includes header fields that may contain information about the protocol version or client software. An example of the GET request format is shown in Figure 2-1. The first line of the figure contains the desired method, then the requested URI, and finally the HTTP version used. The second line contains the request target, which is intended to distinguish between DNS names that refer to the same IP address. There can also be more name-value pairs in the header in the same format.

Figure 2-1. GET request.

In addition to the GET method, other common methods include POST and HEAD. The methods defined in RFC 2616 are shown in Table 2-1 with explanations. The explanation reveals any additional parameters the method may require, typically the URI of the target.

Table 2-1. HTTP methods.

Response

When an HTTP server receives a request, it returns the object specified by the requested URI address to the user, i.e., the rfc2616.txt file in the example in Figure 2-1. Since HTTP does not define the content to be sent, it can be any plain text or binary data represented in digital form. Figure 2-2 shows an example of a response sent by the server. The first line, like the request, indicates the protocol version used, followed by the response code, which in this case is 200. In addition, the response shows date and server information, as well as information about the content's format and size.

Figure 2-2. GET request.

Response codes are categorized by their meaning into "status families" belonging to the same hundreds. Informational statuses belong to the 100s, success statuses to the 200s, redirection statuses to the 300s, user error statuses to the 400s, and server error statuses to the 500s. Common response codes include 404 - Not Found, 500 - Internal Server Error, and 503 - Service Unavailable.

2.1.2 Dynamic web

Presenting static information on websites can be informative in itself, but often some form of interactivity is needed for the user. This may involve various buttons, forms, and other functions that allow the user to influence the content provided by the page in ways other than just through unambiguous state transitions. To achieve this, some logic is needed that receives the user's input and creates a response based on it. This subsection discusses some ways to implement server-side logic, and is based on sources [Pel98, pp. 478 and 660-661].

Common Gateway Interface (CGI)

CGI [CoRo99] is a data transfer interface defined between a server and programs executed on the server. CGI enables the creation of dynamic content for the server's use via an interface, using applications run on the server. Information can be assembled for the page sent to the user, for example, using scripting languages.

The advantages of CGI are simplicity, separate processes, and independence from the server's programming language and architecture. In practice, data transmission through the CGI interface is accomplished using various environment variables and standard IO data streams. Although separate processes can be considered an advantage on the one hand, they also have their drawbacks.

CGI launches its own process for each request intended for it, which is responsible for handling the request and is closed after processing is complete. This procedure consumes server memory and also affects performance due to the process startup and shutdown associated with each request. Multiple concurrent processes can also cause exclusion and synchronization problems if different processes handle variables in the same memory area or a shared data source.

Because CGI is independent of programming language, CGI applications can be implemented in any language, as long as the server's system platform supports it. Most commonly, a CGI application is implemented in an interpreted language such as Perl, Python, or Ruby.

Implementation is also possible with translatable languages, in which case languages such as C, C++, or Java can be used.

FastCGI

Developed as a replacement for CGI, FastCGI aims to address its predecessor's problems, especially with regard to memory usage. However, it retains the good aspects of CGI, namely simplicity and programming language independence. Unlike CGI, processes in FastCGI are persistent, so after handling one request, they remain waiting for the next [OM96]. FastCGI also aims to reduce the number of processes required to handle requests by performing time-consuming I/O operations with the server application.

FastCGI combines environment variables and the standard-IO data stream needed for data transmission into a single data stream, which can be transmitted to local processes via Unix sockets and to processes running on remote machines via the TCP protocol.

API (Application Programming Interface)

To fix problems with other application programming interfaces, many servers also offer their own interfaces for application implementation. The best-known are Microsoft's ISAPI (Internet Server API) [MS95] and Apache API [ASF05]. Also, GoAhead WebServer [GA00], Klone [KL07], and Seminole [GS06a], which are presented in chapter four, offer their own interfaces for application programming.

The main advantage of API applications is speed, as programs linked to a server application operate significantly faster than, for example, CGI programs, because the program operates within the server process and is not started and closed separately for each request. Server APIs often also offer broader functionality than the CGI interface, because it is possible to influence the operation of the server itself during request processing.

A drawback of APIs offered by servers is often the complexity of the programming interface and its limitation to a specific programming language and architecture. Because interfaces typically follow the implementation language of the server itself, they are most often implemented in C or C++. In this case, the application programmer does not have much choice with respect to the implementation language. Complexity is also increased by the incompatibility of API implementations, as the implementations often differ widely. This can cause portability problems for the web application if its implementation is strongly tied to the interface of the server application being used. Furthermore, programming interfaces are rarely open or even free, while the number of open alternatives in the CGI world is countless.

2.2 Server operation

Different HTTP servers use different methods to provide services, but the basic principles of each step, from receiving a request to sending a response, are generally similar. Unix-based HTTP servers primarily utilize the basic methods provided by Unix for inter-process communication and network communication. These methods can be used by functions such as file reading and writing or network communication via socket connections. This section discusses inter-process communication and sockets, and their importance in HTTP servers. In addition, it briefly introduces responding to HTTP requests using Unix system calls.

2.2.1 Inter-process communication

The most common method for processes to communicate with each other is by using pipes.

A pipe refers to a unidirectional communication channel that allows the transmission of a stream of bytes between two processes. Pipes operate on a FIFO (First-In, First-Out) principle, meaning that the outgoing data is read in the same order as it was sent. The data stream transmitted through a pipe does not adhere to any protocol; instead, the protocol used for communication must be defined independently. The system ensures the reliability of data transmission, i.e., that no data is lost during transfer. [Ker02a]

A pipe is created using the Unix pipe() system call. This provides two file descriptors, one of which can be used to read from the pipe and the other to write to it. However, only one of these can be used at a time, so one process is meant to use the descriptor for reading from the pipe and another for writing to it.

Since pipes are unidirectional, two pipes are needed for bidirectional communication. In practice, this means that in communication between two processes, one process utilizes two pipes, one for sending data and the other for receiving. The direction of transmission is determined by using the file descriptor corresponding to the direction. When the opposing process uses the pipes in a corresponding manner, they can enable bidirectional communication between the processes.

Implementing bidirectional communication using pipes requires special precision because they are prone to deadlocks. This refers to a situation where one or more processes become locked, waiting for an event from another process or the release of a resource. An example of a deadlock is a situation where there is a bidirectional connection between two processes, and both processes start reading the file descriptor of the pipe corresponding to reading. In this case, data will never arrive, and the processes will try to read data sent by each other forever. Deadlocks occur most commonly when using blocking operations, as they typically return only when the operation succeeds or an error occurs.

However, the possibility of process locking can be minimized by designing programs in such a way that locking situations do not occur. Unix also offers some system calls to prevent this problem. For example, the select() call can select an active file descriptor by feeding the call a group of monitored read or write descriptors and a time after which their monitoring is stopped. With it, a process can, for example, monitor a group of network connections and assess when it can read from one of them without blocking [RuCo01, p. 154].

2.2.2 Unix sockets

In Unix systems, sockets are most often used as the application programming interface for communication protocols, and they are defined in practice according to either the Berkeley, System V, or POSIX standard. They provide the application programmer access to the transport layer in the form of the TCP or UDP protocol, or directly to the network layer in the form of the IP protocol. This paragraph utilizes Unix system call manual pages [OG01].

A socket is formally defined as the endpoint of an application program's communication and the protocol stack beneath it. In practice, this means that a program can use a socket to read and write data over the network and control the underlying network protocol by setting various options for the socket. From the perspective of an application programmer, a socket is most commonly a bidirectional communication channel over the network, for which a port and protocol to be used are defined. A socket can technically be compared to file descriptors used for disk operations. [Ker02b]

Various system calls are utilized to create and use sockets, the most common of which are socket(), bind(), connect(), listen(), accept(), send(), recv(), and close(). The following illustrates socket operation using these calls. Unix also includes other system calls related to socket management, but the aforementioned are likely the most relevant for illustrating the socket lifecycle and operation. Figure 2-3 clarifies the order of system calls from the server's perspective. From the user's perspective, the block diagram is quite similar, but instead of listen() and accept() calls, the connect() call is used to establish a connection, and typical data transfer operations recv() and send() occur in reverse order. Common to all calls is the numerical value they return, which can be examined to determine the success of the operation.

Figure 2-3. Network operation using sockets from the server's perspective.

Sockets are created using the Unix socket() system call, which takes the address family, socket type, and protocol to be used as parameters. The address family used is usually the AF_INET address family for IP addresses. The socket type can be, for example, SOCK_STREAM for TCP traffic and SOCK_DGRAM for UDP traffic.

The protocol is usually zero, which means the default protocol for the socket type being used. Once a socket is created, it can be handled using the descriptor returned by the call. Data can also be handled as is, without a protocol, if necessary.

Once a socket is created, it needs to be bound to a desired connection. This defines the local address and remote address, i.e., the endpoints between which the socket will communicate. This is done with the bind() call, which takes the socket to be bound, the address, and the address length as parameters. In the case of a listening port, the remote address of the connection is usually left open. The address format must conform to the address family used by the socket, and in the case of an IP address, it must define the IP address and port to be used.

Next, it must be chosen whether to connect to another endpoint or put the socket into listening mode. Connecting to another endpoint is done with the connect() call, which requires the socket being used, the address, and the address length as parameters. Listening mode can be set with the listen() call, which takes the desired socket and a backlog value as parameters, the backlog value representing the maximum length of the connection request queue.

After binding and connecting or after a connection request, the socket can be used to read and write data. This is done using the recv() and send() calls, which take as parameters the socket to be used, a character buffer, the length of the buffer, and additional settings. When reading, the received data is stored in the character buffer, and when writing, the data to be sent is read from it. In Figure 2-3, the server uses the recv() call to read the request and the send() call to send the response.

It should be noted that the recv() and send() calls are intended for connection-oriented TCP traffic. For transmitting datagram-based UDP traffic, the sendto() and recvfrom() calls are used, which require the recipient or sender address as an additional parameter. When using UDP, the connect() and listen() calls are not used because sending and receiving occur without a separate connection establishment.

When a connection is to be terminated, the close() call is invoked. Its only parameter is the socket to be closed. Close() is a general-purpose call for closing file descriptors, so it can be used to close, for example, a file descriptor in addition to a socket.

2.2.3 Responding to the Request

The connection to the server begins with a command initiated by the user when they type the desired server address into a browser. After this, the HTTP client program establishes a TCP connection to the server software [Pel98, p. 19]. Once the connection is opened, the transmitted HTTP request header specifies the requested content as described in subsection 2.1.1. If the data requested in the header is static, such as an HTML file, it can be retrieved from the server's file system. Otherwise, the server generates the content dynamically.

Usually, the first step performed by a server program is to start listening to the desired TCP socket. Listening involves receiving incoming connection requests on the port, then accepting the request and reading its request header. After that, the necessary steps are taken to create a response. This may involve, for example, reading from disk or making database calls.

At its simplest, generating a response means sending an HTML file saved on disk, but dynamic web applications require more complex logic, such as that discussed in section 2.1.2.

When planning server operation, it's important to consider blocking of various operations, as mentioned in subsection 2.2.1. Generally, operations that read data or receive connections block. Blocking occurs, for example, in the Unix accept() call when an acceptable connection has not yet arrived, or when reading data with the read() call. An efficient HTTP server must take blocking into account so that it does not cause the entire server to stop, which would be seen by the user as a poor response. The problem can be solved with child processes started with the fork() call, threads, or event-based design, which can be aided by, for example, the select() call, which selects an active file descriptor. Different server process models are discussed in more detail in the following section.

Once a request is received, its processing can be directed to a child process or thread corresponding to the request, which performs the actions required to handle the request. These may include evaluating the parameters provided by the request and, for example, performing database calls based on the parameters. The response is typically sent to the user using a send() call, after which the socket can be closed or left waiting for the next request.

2.3 Process models

This section discusses various process models from the perspective of an HTTP server. The basic principles of the models apply to any process, but in this section, the stages used as the basis for examining the models only occur with a program characterized by receiving a request, processing it, and sending a response, just like an HTTP server. This section is based on the source [Dru99].

From a project perspective, the process model of a server application is important in terms of its memory usage and integration with the target environment. From the operating system's point of view, process models mainly differ in the number of processes, as, for example, a multiprocess application uses several concurrent processes, while a multithreaded or SPED-model application usually manages with one. On Unix-based operating systems, the creation of new processes is made more efficient by the fork() call and, consequently, copy-on-write for data sharing [HaJä03, p. 144].

The impact of the process model on the application's memory capacity requirement is therefore small, because virtual memory pages are allocated according to the use of variables, making the application's implementation and its actual load the more important factor affecting the memory requirement. Nevertheless, even a small memory saving offered by the process model was considered beneficial in the project, which favored minimizing the number of processes.

The ease of integrating the process model into the project's target environment is of particular importance. In the target environment, the interleaving of time-consuming operations of various application modules is done in the manner described in 3.2.1, by listening to file descriptors corresponding to the modules. Increasing the number of processes also increases the number of file descriptors to be listened to due to process-specific resources, so this fact also supports minimizing the number of processes used.

Multiprocessing model

In a multi-process model, a process is assigned to each user request, which executes the sequential steps associated with that request. The process completes all steps related to one HTTP request before accepting a new request. This typically means in practice that one process handles receiving requests and starts a sub-process (child process in Unix systems) that handles the individual request. When multiple sub-processes are started, multiple HTTP requests can be served simultaneously. If a process becomes blocked, for example, due to a disk or network operation, the operating system switches the process being executed, so that processor time is not wasted.

Each process also has its own address space, which makes it easier to execute concurrent HTTP requests, as each process can maintain the state information of the request being processed without worrying about conflicts between requests. However, implementing optimizations based on global data, such as cached URLs, may be difficult. Most HTTP servers also require some kind of database, in which case the requests, saves, and other operations made by the processes must take place taking into account the integrity of the database. This usually requires solving exclusion and synchronization problems through inter-process communication.

Multithreaded model

A multithreaded server operates on the same principle as a multiprocess server, but to conserve resources, threads handle requests instead of processes. When one thread blocks, for example due to an I/O operation, the program's execution can continue in another thread. The most significant difference between a process and a thread is in resource usage: instead of thread-specific resources, threads within the same process share, among other things, the process's memory space and file descriptors. This reduces the amount of resources needed and speeds up context switching between different threads. Each thread's local variables are stored in its own stack memory. Controlling threads usually requires utilizing shared state information and synchronizing access to shared data. Like processes, one thread performs all the necessary steps after receiving and accepting one HTTP request before accepting a new request.

Using threads requires operating system support, and the benefits gained from threading partly depend on the implementation of that support. Some operating systems also offer user-space thread libraries, in which case actual kernel support is not required, but the performance benefits do not reach the level of native threading.

Single-process Event-driven (SPED)

The SPED model differs from the previous two models in that, instead of a process or thread being started for each HTTP request, a single process is responsible for both receiving and handling requests. The execution of this server process must occur in such a way that it never enters a locked state, which would interrupt the server's operation. This requires using blocking system calls in a way that maintains server availability. Additionally, instead of blocking I/O operations, their asynchronous counterparts can be used, if the operating system supports them. In Unix systems, it is useful to use the select() call, mentioned in subsection 2.2.1, in connection with blocking calls. The principle of this call is that, instead of waiting, which excludes other activity, it periodically checks whether the I/O operations to be performed are active, so that processor time is not spent waiting for a single blocking operation. In addition, some blocking calls can be given an additional parameter that causes them to return immediately, and the success of the operation can be determined from its return value.

The operation of the SPED server is based on maintaining request-specific state information, allowing each request to be processed in segmented stages, one step at a time. In each iteration cycle, the server can check incoming connections, transmit buffers, or completed file operations and select the active source to execute. After execution, the next stage of that request is initialized, which could be, for example, sending an HTTP response.

A significant problem with SPED servers is the processor time-consuming I/O operations, such as reading a large file. The server process should not block when executing a single request, as this prevents new requests from being received, potentially resulting in poor availability or response time for the user. In modern operating systems, many CPU, disk, and network operations can also be overlapped or executed in slices. This further enhances the performance and especially the response time of the SPED server, but the problem is the lack of support in older operating systems. For example, asynchronous disk operations are a standard feature in systems using the Linux kernel 2.6.

Despite support for asynchronous operations, most SPED servers do not utilize them, instead relying on traditional system calls and their provided concurrency management. Consequently, SPED servers are generally used only for specific reasons due to their memory saving benefits. They are particularly well-suited for embedded systems with limited memory capacity.

Asymmetric Multi-process Event-driven (AMPED)

The AMPED model aims to leverage the essential features of the event-driven SPED model, namely state-based execution and shared memory, and to support it with multiple auxiliary processes or threads. Auxiliary processes can handle, for example, necessary I/O operations. They help to improve the server's response time because blocking disk and network operations can be handled in their own processes while the event-driven main process handles the other stages of HTTP request processing.

The AMPED model aims to maintain the performance of the SPED model in operations other than file reading, but avoids its problems regarding, for example, slow disk operations and system support issues for asynchronous disk operations.

The AMPED model can be utilized with conventional operating system methods, so there is no need to compromise on ease of deployment and broad system support. In Unix systems, AMPED uses standard system calls such as read(), write(), and accept() when using sockets and pipes, as well as the select() or poll() call to test for I/O operation completion. When using the AMPED model, managing file system data access is also essential, just like with multiprocess and multithreaded models.

Although the AMPED model isn't as popular as other process models presented in this section, it is a server option to be seriously considered, especially for embedded systems where good performance is required from the software with a small load.

2.4 Server Security

Implementing any server application often inherently includes security. In the case of a WWW server, this usually means access control for the server's documents and, in applications requiring greater privacy, such as banking services, also encrypting the transferred data. This section discusses a few of the most common security methods that arose during the project, related to both user authentication and data encryption.

The authentication methods covered include HTTP standard authentication, as well as SSL (Secure Sockets Layer) and its successor TLS (Transport Layer Security) for stronger authentication. A common way to implement strong authentication is the public key method used by SSL. In connection with SSL and TLS protocols, the means they offer for encrypting data are also examined.

Server-specific security solutions, such as IP-based access control and file system-based permission restrictions, are excluded. A server application can also implement its own authentication method, allowing the management application to verify the user's authenticity. These aspects will be discussed later in chapter four when introducing servers. Furthermore, this section only covers the security between the user and the server application, even though a secure server implementation must also consider the server architecture, the security of the software code, and the company or organization's overall information security.

Based on user authentication, it is usually also necessary to perform some kind of authorization, which defines the user's rights to server functions. In the SHDSL device management application, this means user-specific device management capabilities. In this case, authorization affects only the implementation of the web application to be implemented, rather than the server software, so it will be revisited as necessary in chapter five.

2.4.1 HTTP Authentication

The HTTP standard introduces a method for simple access control, i.e., authentication. The standard includes two authentication schemes: the simple basic scheme and the more secure digest scheme. The IETF has defined these methods in more detail in RFC 2617 [Fra99]. This subsection discusses the authentication methods offered by HTTP and is based on the aforementioned RFC in addition to sources [Ker98, pp. 148-153] and [Pus01, pp. 140-153].

HTTP authentication is based on identifying a user with a username and password and can be used to protect documents on a server. Although using passwords is a weak authentication method due to human risk factors, it is strong enough for many applications. In more demanding applications, only using a password in conjunction with a strong authentication method, such as SSL, is sufficient to guarantee the required data security.

Basic authentication

The server can be configured to use basic authentication, which is usually done in the server's general configuration file, where the authentication to be used and the allowed usernames are defined on a directory-specific basis. User passwords can be set in the same file or in a separate password file, which is usually also encrypted.

When a user establishes a browser connection to a protected folder on a server, the server responds with a message indicating that accessing the folder requires user authentication. Figure 2-4 presents an authentication request from the server when basic authentication is used. It shows, in addition to the concepts familiar from subsection 2.1.1, the response code 401 indicating a usage restriction, the authentication method used (basic), and the realm "salainen" (secret). The latter can be used to define "security realms" within which the user only needs to authenticate once.

Figure 2-4. Server basic authentication request.

Based on a request, the browser opens a popup window where the user can enter their username and password. The browser sends its GET request with the username and password in the Authorization header field to the server, to which the server responds with a standard OK message and the associated data if authentication is successful.

The weakness of basic authentication is that the username and password are sent in the GET message in plain text, allowing an attacker eavesdropping on the traffic to easily gain access to the credentials. However, this may be a requirement for some applications if the password received from the user must be compared in plain text.

Digest authentication

The digest method helps with the shortcomings of Basic authentication, its purpose being to calculate a hash of the data to be transmitted over the network based on given random values. The digest method also does not address encrypting message payloads, but as an authentication method, it corrects the biggest problems of the Basic method.

In operation, digest authentication is similar to basic authentication, meaning that when a user requests a document from the server without the correct permissions (correct username and password), the server responds with a 401 message indicating a user restriction. An example of the message is shown in Figure 2-5.

Figure 2-5. Server digest authentication request.

The image shows the realm to be used, the quality of protection, a random string for calculating the digest, and an opaque string that the user must return unchanged to the server in the digest. The MD5 function is used for calculating the digest.

After the user has returned the calculated digest, including the username and password it contains, to the server, the server calculates the same digest itself and compares the result with the digest received from the user. If the digests are the same, the user's authenticity can be verified and an OK message and the requested data can be sent to the user. It is no longer possible to decrypt the digest back to plain text, so using the digest method requires the server to know the correct password in its original form. Therefore, for example, comparing digests to encrypted Unix passwords is not possible because converting them into a comparable format is practically impossible.

2.4.2 SSL/TLS

HTTPS is most commonly implemented using the SSL protocol, which enables secure data transfer over the network. SSL is the most common and perhaps most important internet security protocol [Ker98, p. 296]. SSL is a protocol developed by Netscape, and version 3.0 is currently used [Fre06]. The successor to SSL, the TLS protocol, is also widely supported, and the abbreviation SSL is often used as a generic term for these two protocols. TLS is developed by the IETF, and version 1.0 is most commonly used [DiAl99], which was released in 1999. In the spring of 2006, TLS version 1.1 was released, which has improved, among other things, the operation of the RSA algorithm.

Both SSL and TLS ensure three fundamental security properties for data transmission: authenticity, confidentiality, and data integrity. Confidentiality means that information in or transmitted by a data system is only available to those authorized to access it. Authenticity means that the parties involved in communication can be identified, and integrity means that the data is reliable. The changes in the TLS protocol compared to SSL are small, and it supports the message structures defined in SSL 3.0 well. TLS has also removed support for the Fortezza encryption card. [Ker98, p. 93]

SSL protocol

The SSL protocol operates between the transport and application layers and is independent of the application using it. SSL requires its own TCP/IP socket, for which port 443 is commonly used. For example, for an HTTP server using SSL encryption, this means using the SSL port instead of the common port 80. When SSL is used, all traffic passing through it, including the document URL, its content, form content, cookies, and HTTP headers, is encrypted. To a casual listener, only the IP addresses of the communicating parties and the type of SSL messages are revealed.

SSL enables flexible selection of symmetric encryption, digest, and authentication methods. SSL can use DES, 3-DES, RC2, or RC4 algorithms for encryption and MD5 or SHA digests as well as RSA keys and certificates for authentication. Communication can also occur anonymously, in which case the Diffie-Hellman key exchange algorithm is used. [Ker98, p. 297]

SSL transaction steps

Next, we will go through the steps performed during an SSL handshake between a server and a user at a general level. At the beginning of the SSL handshake, the endpoints of the data transfer are authenticated, and a symmetric key is agreed upon for use in encryption. The steps are performed using messages sent between the endpoints, the flow of which is illustrated in Figure 2-6.

Figures 2-6. Establishing an SSL connection [Maj05].

The user initiates communication with the server with a Client Hello message, which includes session information, random data for encryption, and information about the protocol and security methods supported by the user. It can also be a response to a Hello Request message from the server, with which the server can express its request to initiate a session negotiation.

The server responds to a successful user connection with a Server Hello message. If an error occurs, an error message is returned. The Server Hello contains information about the encryption and compression algorithms specified by the server, and the server's own random data for encryption. In addition, the server checks the session ID possibly provided by the user, and returns either a value indicating a new session or a value indicating an old, previous session, as needed.

Next, the server sends its certificate to the user, which conforms to the selected encryption settings. Generally, an X.509.v3 standard certificate is used [Ker98, pp. 150-152]. The user knows that the server handshake phase has ended based on the Server Hello Done message. If the server has made a certificate request, the user sends the server their own certificate, which has the same format as the server's certificate. With the help of the certificate, the server can verify the user's authenticity in the same way that the user verifies the server, thus achieving authentication of both endpoints.

After this, the user sends a Client Key Exchange message, which contains the key used to create the final session key. This key is called the premaster secret, and its format and length depend on the selected encryption algorithms. The final session key, or master secret, is used to encrypt all data transferred in the session. Encrypting the premaster secret using PKI (Public Key Infrastructure) before sending it is paramount to ensure the transferred data can be encrypted from an attacker.

Sending the session key is followed by an optional Certificate Verify message to verify the certificate sent by the user. The connection establishment is finalized with Finished messages sent by the connection parties to each other. This also confirms the correctness of the key exchange and authentication events using the digests contained in the messages. After this, the actual session can begin, during which the endpoints encrypt the traffic passing between them with a shared, symmetric encryption key.

2.5 MVC architecture

While the requirements for web software in terms of versatility, appearance, and user-friendliness are increasing as data transfer connections and server computers become more efficient, their implementation is becoming more difficult year by year. An additional challenge in the implementation of web software is the large number of interfaces it uses, and sometimes also programming languages, because the software communicates with the outside world for users and often utilizes database software.

Improved development environments, offering error-checking features and enabling the handling of different software parts in a unified manner, help with increased complexity. Furthermore, a prevailing trend in software development is to break down large software systems into smaller, more easily understood parts, which can also be developed independently of each other if necessary. Modular design is enabled by, among other things, object-oriented languages Java and C++, as well as hierarchical design of the call structure in procedural languages.

Especially in network programming, the modularity of the programming language alone is not sufficient, simply because modern web applications may utilize several different programming languages: HTML, JavaScript, and CSS can be used for the appearance, Java, Perl, or PHP for the logic, and SQL (Structured Query Language) calls or another interface may also be needed for database handling. The MVC (Model-View-Control) architecture aims to separate these parts from each other, so that, for example, SQL calls do not need to be written directly as parameters of an HTML form, and on the other hand, the SQL database does not need to worry about the appearance of the response sent to the user. The MVC structure is generally suitable for all software types containing user interface, logic, and database, but within the scope of this project, it is necessary to examine it from the perspective of a web application.

Numerous application frameworks based on the MVC architecture have been implemented for various languages, and there are also several terms used for the architecture. Sun refers to MVC as Model 2, which is utilized by Sun's J2EE (Java 2 Platform Enterprise Edition) application framework. Other well-known MVC application frameworks include Struts designed by Apache for Java, Ruby on Rails for Ruby, Django for Python, and ASP.NET developed by Microsoft. There are also several application frameworks designed with the MVC architecture for PHP.

This work does not address specific application frameworks because they were not utilized in the project's implementation. Instead, the HTTP server's own API and a templating engine were used, which will be discussed in chapters four and five. As noted in the mapping of server options in chapter four, most servers support CGI or FastCGI interfaces, which would facilitate the deployment of application frameworks created for interpreted languages, among other things. However, these technologies were approached with caution because application frameworks are generally heavy and therefore not suitable for the project's embedded network terminal. Nevertheless, this work aimed to leverage the principles of the MVC architecture to divide the application's various components into separate, as independent parts as possible. This achieves better program maintainability, portability, and reusability of its components [HaMä98, p. 355].

Notable application architectures besides MVC include Simple Separation, which divides the application into program and user interface parts, and Microsoft's MFC (Microsoft Foundation Classes), which is based on the Document/View separation.

As its name suggests, MVC architecture divides software into three distinct parts: Model, View, and Control. The Model refers to the software's persistent, application-specific data, the View to the user-visible part, and the Control to the logic that operates between these two parts. A simple block diagram in Figure 2-7 illustrates the interaction between the different parts.

Figure 2-7. MVC architecture.

In the image, a solid arrow represents a direct connection between parts, and a dashed line represents an indirect connection between them. In practice, this means that all commands from the view to the model and, conversely, all data to be presented from the model to the view go through the controller. The MVC architecture enables software design in such a way that changes in one part of the software do not affect the rest of the program's functionality, provided that the interfaces between the different parts remain the same.

Model

In the MVC pattern, the model refers to domain-specific, persistent "real-world" data, such as accounting entries in a billing system or user information in a web forum. The model itself may contain the logic required by the data it stores, providing the upper layer of the application with an interface for accessing and manipulating the data. In addition, the logic helps to keep the data organized, so that the upper layer does not have to worry about the structure and constraints of the data being processed. In more complex data structures, it is also necessary to take care of data referential integrity and transaction synchronization.

The most common model is a relational database using SQL, such as MySQL or PostgreSQL. However, the model can also be, for example, a simple file used for storage or a physical device manipulated by software.

View

For the user, the most familiar part of a web application software is the view. The view's task is to relay commands received from the user to the control and to take care of presenting data received from the model through the control in a format understandable to the user. Information received from a lower level does not necessarily have to come from the model; views can also be created directly based on responses received from the control. However, most commonly, events also involve processing data residing in the model.

A typical web view uses HTML for presenting data, and possibly JavaScript for view logic and CSS or XSL for fine-tuning the appearance. The main benefit of isolating the view from the rest of the program is that when designing other parts, there is no need to consider view-related aspects such as data layout or page appearance. Furthermore, the web page designer does not need to master the implementation language of the application logic underlying the view, but only the markup and scripting languages used for creating web pages.

Control

Between the model and the view, we still need the program's brains, which are called a controller in the MVC architecture. The controller receives requests from the user and provides an appropriate response using the model as an aid. The controller knows the services required by the view and offered by the model, but does not take a position on their implementation methods, but rather acts as the necessary logic between these parts.

The controller's operation is usually based on the necessary handlers and callbacks that the view can use. The handler receives a call and its required parameters, based on which it uses the model's interface to retrieve the data needed by the view. If necessary, the controller can manipulate the data, for example, by removing specific data fields if it knows that the view will not need them.

The controller also shapes potential errors in logic or the model into a response understandable by the view, for example, by indicating an erroneous value entered in an HTML form. Thus, errors occurring in the lower layer can be presented to the user in the desired way. Callbacks can be used to define the information presented in the view. Callbacks are usually various loop, conditional, and evaluation commands, based on which the information received from the model is printed to the view. Thus, the view can freely define the information presented to the user and its layout.

3 Target Environment

In defining and designing an application to be implemented on a network terminal device, the constraints imposed by the target environment are important. In embedded systems application development, this is especially important because the hardware is generally limited in performance and the application environment is modest and predetermined in terms of features. The computing power and memory capacity offered by the hardware provide an understanding of the application's requirements and limit, for example, the solution options for the HTTP server used as the core of a web management application. Non-functional requirements related to the response times and usability of the management application should also be reflected in the hardware capabilities. Even more important than the hardware, a limiting factor in the application's implementation is the existing application environment, which provides the operating system, compilation environment, programming interfaces, and other applications and their interfaces used in application development. It is also advisable to try to adhere to the general design principles of the application environment, although flexibility from the principles is also possible in exceptional situations. The design principles include, among other things, the recommended single-process and threadless process model.

One aspect related to software implementation that is not directly related to the target environment is the version control method used for storing, maintaining, and managing changes to the software code. Version control of the source code tree for the Iris 440 device application environment is discussed at the end of this chapter.

3.1 Hardware

The functionality of the Iris 440 terminal device is implemented with various controller circuits and the processor itself. The device's core functions are running the operating system and receiving and transmitting data traffic to the correct line in the appropriate format. This point is based on source [DC06].

The Iris 440 device is a customer-premises equipment (CPE) and point-to-point SHDSL terminal device, which provides the customer with an internet connection through a symmetrical and fast SHDSL line. The device is shown in Figure 3-1. The device includes four Ethernet ports and a serial port for management connection. The maximum customer connection speed is 22.8Mbps and the Ethernet ports are 100Mbps.

Figure 3-1. Iris 440 terminal device.

The main components of the device are the processor, FPGA chip, and the switch and line circuit responsible for the Ethernet and SHDSL interfaces. Figure 3-2 shows a simplified block diagram of the different parts of the device and their interconnections.

Figure 3-2. Iris 440 device HW architecture.

Traffic passing between the device's Ethernet and SHDSL interfaces is always routed through the device's processor. In addition, the data path includes an FPGA chip, one of whose functions is to act as an adapter between the processor and the SHDSL line circuit.

CPU

The device's CPU, or central processing unit, is 32-bit and operates on the MIPS instruction set. It operates at a 300MHz clock speed and includes 16 kilobytes of cache memory. The processor also includes a DMA and PCI controller, as well as two MII interfaces. Other interfaces include UART, I2C, SPI, and JTAG. The UART interface is used as a control bus in the device, and the I2C bus is used for handling EEPROM memory.

Memory

The device includes DDR memory where software code is executed and which acts as a temporary data store. The total memory is 256 megabytes and is divided between two circuits operating on 32-bit memory buses. For permanent storage, the device includes 64 megabytes of flash memory. This stores, among other things, the bootloader, a Linux disk image, and configuration files. In addition, the device includes a small EEPROM memory chip, which is used as a storage location for electronic identifiers.

Ethernet switch

A switch is needed for the Ethernet ports on the device to connect the ports to the device's processor. This circuit therefore acts as a hub between four RJ45 ports and the processor's MII interface.

SHDSL line circuit

In addition to Ethernet connections, SHDSL lines also require their own line circuit, which distributes the data stream from the FPGA to its own lines. Iris 440 includes four SHDSL lines, which are combined into one fast, user-facing modem line.

FPGA

The purpose of the FPGA circuit is to transmit the Ethernet packet stream and control commands from the processor into a data stream suitable for SHDSL lines and vice versa. In addition, the FPGA acts as an LED controller for non-Ethernet LEDs.

3.2 Application environment

The application environment of the Iris 440 device refers to its operating system and the processes run on it, as well as various tools. Processes include process modules responsible for managing the device's features and communicating with the environment. Tools include basic Unix tools and various applications used for compiling and managing binaries. This section discusses the application environment from the perspectives relevant to the project. In particular, examining the operation principles of the programming interfaces and process modules used is important because they significantly affect the design and implementation of the web management application.

3.2.1 Operating system

The Iris 440 device operating system consists of the Linux kernel and Unix tools provided by BusyBox [Per96], intended for embedded systems. In addition, the device runs numerous different process modules, each responsible for some task related to device management or communication. Process modules include, for example, modules providing a management interface for the device's bridges or network interfaces, as well as management modules that negotiate with the user via a command-line or HTTP connection.

Kernel

The Iris 440 device uses a modified version of the Linux kernel 2.4.20 as its operating system kernel. The kernel is the core software component of a Linux system, and its features largely determine the capabilities of the entire system [Yag03, p. 156]. The kernel's job is to communicate with the system's hardware and provide applications with a standard interface for utilizing different devices. The kernel also handles memory management for applications running on the system and ensures that each application receives the amount of processing time it needs from the processor.

Device drivers using different device interfaces can be included in the kernel either built-in or as separate modules. One of the strengths of the Linux kernel is its modularity, which allows modules to be loaded or unloaded as needed, and the size of the operating system kernel in central memory remains small. This is especially important for embedded systems. Because the memory capacity of an embedded system is limited, it is also advisable to remove unused parts from the kernel. The parts included in the Linux kernel are revealed by the .config file used to compile it, which in the Iris 440 device only includes the most essential parts. Most device-specific drivers are compiled separately as kernel modules mainly because their rarity or closed code means that the default Linux kernel does not offer them.

Process modules

Process modules are programs designed for a specific task, offering their own interface for use by the rest of the application environment or the outside world. Embedded systems are characterized by real-time processes, the execution of which differs from traditional systems in terms of timing requirements and concurrency management [HaMä98, p. 306]. The inputs and data streams processed by the system are unpredictable, which necessitates specific requirements compared to traditional batch-based applications. Real-time systems may also have parts that process batch-type inputs, and these are called passive modules.

In the Iris 440 device's application environment, the challenges posed by real-time operation have been solved through inter-process module communication and their asynchronous operation. Inter-process communication is implemented using an IPC (Inter-Process Communication) interface, which is realized by a separate IPC module. The IPC interface allows processes to transmit data and requests to each other, and it can be used to take care of process synchronization. In this case, asynchronous operation means that each module registers itself with the system's operation-controlling monitoring module, to which the module can report its activity, for example, in connection with an I/O operation. A module can become active, for example, when input arrives from the user.

The advantage of module asynchronicity is that computing power is not wasted while a single module waits for an event to occur; instead, that computing time can be used elsewhere, and the module signals its activity as needed. Module registration with the monitoring module is implemented using a special class from which the module inherits its own listener class. This class must implement a specific method that the monitoring module calls when the module is active. This method initiates the execution of the function implemented by the module, which may include reading and processing I/O input, for example. Registration of the listener object with the monitoring module is done through the monitoring module's `register_fd()` method, to which the listener provides the file descriptor to be monitored and itself as input.

In the monitoring module, each module is mapped to its corresponding file descriptor, whose activity can be monitored using the select() system call. Based on the activity of the file descriptor, the execution method of the corresponding listener object can be triggered, and thereby the function provided by the module is executed. Due to the scheduling controlled by the monitoring module, only one module at a time is responsible for communicating with the device's resources. This avoids, for example, the problem of excluding simultaneous management requests from two different management processes, because only one process can handle a specific management object at a time. This reduces the need for inter-process communication and prevents complex concurrent programming problems. To simplify matters, the process model instructed for use by individual modules is single-process and event-driven, preferably not thread-based. If multiple processes or threads are used, exclusion problems would also have to be taken care of within the individual module. The advantages and disadvantages of different process models were discussed in section 2.3.

3.2.2 Config API

Config API is a management interface originally created by Design Combus Oy for the Iris 800 DSLAM (Digital Subscriber Line Access Multiplexer) device. It allows reading and modifying the device's settings. The interface includes methods for adding, updating, deleting, and reading different management objects. These calls can be used to manage the device via a management application. Currently, the Iris 800 device is managed only through a CLI application, but for the Iris 440 terminal device, it is also planned to enable management through a web-based management application.

CLI and web management applications are parallel interfaces for device management, each containing its own logic for mediating inputs and responses between the user and the Config Manager. For reuse purposes, it is most sensible to make the management applications as consistent as possible with each other, although due to the different nature of terminal-based and web interfaces, many application elements differ from each other already in the design phase.

Management objects and invocation logic

Manageable items located on the device appear to the application as management objects.
Management objects are aspects of the device that can be inspected or modified, such as a bridge, network interface, or the IP address of the DNS server used. Each object has a certain number of attributes, the most essential of which are the key attributes used to identify the object. Some objects do not contain any key attributes at all, which in practice means that there can be at most one such object.

For each management object, a home is implemented that defines the event to be executed when a method of the object is called. The different methods are add, update, get_first, get_next, and delete. Each home is registered with the Config Manager, which is handled via API methods. The handling of home interfaces and their corresponding manager applications through the Config API interface is illustrated in Figure 3-3. Although the methods of the interface and their inputs and responses are essentially the same for all objects, their implementations vary due to different hardware and driver interfaces. For this reason, homes must be built taking into account the specific characteristics of the object. Home implementations are managed by the home factory, which acts as an abstract interface for objects, which the management application can use to manipulate data. The home factory directs the received request to the correct home, allowing management objects to be handled with the same methods regardless of object type.

Figure 3-3. Handling manager applications using the management application.

Due to the mechanics described above, the higher-level management application does not need to be aware of the homes registered in Config Manager, and the requests it sends can be arbitrary. The dispatcher is responsible for receiving calls to the Config API, which directs them to the correct home registered in the home factory, thus directing the call to the desired object. Thus, the management application itself only needs to evaluate the response, as the Config Manager and the home corresponding to the object take care of directing the methods to the correct object in the correct way.

Object parameterization

A user-friendly management application requires the necessary amount of metadata to be presented to the user about various management objects. It is necessary to be able to supplement the basic properties of management objects and their metadata in a unified manner. The solution for this is to use def files, which are a simple list of attributes, whose definitions are name, data type, default value, mandatory status, and constraints. Figures 3-4 show a snippet of the def file for the i24_interface object, which defines the management attributes of the device's network interface. For example, the type attribute has a data type of string, a default value of empty, and is mandatory. Possible inputs are ethernet or shdsl.

Figures 3-4. Definition for the i24_interface object.

Control objects defined in def-files and the HOMEs created with them offer the possibility for generic implementation of management applications. When each control object is defined with a uniform syntax, they can also be handled in a uniform manner. From the perspective of a management application, creating a new object, in its simplest form, means creating a new def-file for the object, which is used to generate an interface for handling the values defined for it. In addition, maintaining object changes is easy because each change, such as adding an attribute, is made in only one place.

3.2.3 CLI management application

The Iris 440 device, like the Iris 800 DSLAM device, uses a CLI management application that is based on the Iris 800's CLI program code. Although the web management application discussed in this work differs in functionality from the CLI, it can utilize some of the same design principles. For this reason, a superficial examination of the CLI's operation is also appropriate.

The Iris 440 CLI is accessed by logging into the device with admin credentials, which automatically starts the CLI application. You can also log into the CLI with monitor credentials, which are intended for device monitoring. The CLI management application follows the general operating principles of similar applications, one example of which is Cisco IOS. Its features include management object-specific configuration modes and "no-" commands that act as overriding commands, which are used, for example, to delete an object.

Each type of CLI command, such as adding, deleting, or viewing an object, corresponds to its own C++ class, and different CLI commands are instances of these classes. Commands are created when the CLI application starts and can be defined as needed. The syntax of the commands is defined as parameters when the object is created. Figure 3-5 shows an example of a command that creates a new add command, which can be used to configure a new bridge for the device, i.e., create a new bridge object.

Figure 3-5. Example of a command definition.

The comment creates a new CCliAddCommand object that defines a command with the syntax "bridge name [name]". The command is for users in admin mode and also specifies a configuration mode index (ENABLE_MODE_TREE) and a configuration mode after the command is issued (BRIDGE_CONFIG_MODE). Thus, the command is available in the application's main mode, and after issuing the command, the CLI enters the configuration mode of the added bridge. Help text is also defined for each parameter during object creation, which serves as a guide for the user. The help text appears in the CLI by pressing the TAB key or with a question mark.

Because commands pertaining to different object types are created with the same generic classes, the amount of required program code is reduced compared to creating the logic implementing each command separately.

The functionality of the comment classes corresponds to the interaction with the Config API, i.e., the device configuration interface. In the case of the add class, this means an add call directed to the object's home with specified parameter values as shown in Figures 3-6. The parameters of the object to be saved are set using the SetAttrValues() method provided by the user.

Figure 3-6. Object storage via the Config API.

In addition to the add command category, other CLI command categories include delete, update, display, reset, and configuration file saving command categories. The fewer categories an application operates with, the less program code is required to implement the CLI in practice. On the other hand, as the number of command categories decreases, consideration of object-specific properties and thus the user-friendliness of commands also decreases. For this reason, properly estimating the number of categories is a balancing act between minimizing workload and ensuring sufficient usability. The same is evident in web management application design.

In addition to the command classes, their functionality, and command definitions, the CLI also requires the main part of the application, which is responsible for listening for signals coming to the CLI and directing their processing to the application logic. This part is also responsible for registering with the monitoring module as described in section 3.2.1.

3.2.4 Compiling

Both traditional and embedded software designers require different compilers, linkers, and interpreters to translate software code. Embedded software translation tools differ from traditional ones in that the software is usually compiled in a different environment than where the final binary is executed [Yag03, p. 109].

The target environment for this project runs on a MIPS processor, so the resulting binary must be machine code understood by the MIPS processor. Because the workstations used for development are x86-based, cross-compilation is required, which means compiling the program code with a compiler intended for the target environment and using binary tools and libraries supported by the target environment.

The Iris 440 device's application environment consists of the operating system and various process modules that communicate with the device and its environment, as described in subsection 3.2.1. The program code required by each module is located in its own folder in the application environment's code tree. For example, the software code for the web management application to be implemented is placed in the dslam/src/dcombus/web/ folder. In this case, the code tree refers to the directory structure that contains all the program code, translation files, and pre-compiled applications and libraries required by the Iris 440 device.

The Iris 440 device software is compiled using the GNU project's make command-line tool [FSF97], for which the program parts to be compiled can be specified in the Makefile used by the tool. The file specifies the necessary tools, settings, and files for compilation, so that the compilation itself happens with simple command-line commands, and the user does not have to worry about the details of the compilation.

For example, compiler and linker settings can be defined in variables, making it easier to modify the settings. Makefile files can also be chained, meaning that each directory in the directory structure can contain its own compilation file, which is automatically called during a compilation call made from the root directory, as long as the subdirectory is defined in the root directory's compilation file. The compiler used is mipsel-linux-gcc, which is a MIPS-platform-specific version of the GNU project's GCC compiler.

Each module's root directory contains its own Makefile, which is simple and contains only two targets: all and clean. The all target means compiling the module, and clean means removing it. An example of the file content is presented in Figure 3-7. A target refers to a target parameter defined for the make tool.

Figure 3-7. Makefile compilation file of the web process module.

The build-web and clean-web commands used in Figures 3-7 are targets, such as all and clean, and their content is defined in the module.mk file. This file is part of the Makefile, which has been isolated into its own file to separate user and compiler targets. The module.mk file contains information about the files to be compiled, subdirectories, outputs, dependencies, and settings. When dealing with module-specific compilation files, you should strive to minimize the configuration of compiler settings yourself, as the largest possible portion of the settings should be able to be set from the root directory of the source code tree. This makes it easier to compile source code for different target platforms, for example, because changes to the compilation settings only need to be defined in one Makefile file.

Each subdirectory containing module source code also has its own module.mk file, which lists the files contained in the subdirectory. These module.mk files in the subdirectories, the module.mk file in the root directory, and the Makefile itself together form the actual compilation file, based on which the all and clean commands given to the user's make tool are executed. In reality, the user only needs to give the make all command to the root directory of the entire application environment source code tree, in which case the compilation files contained in each subdirectory are executed in a chained manner.

3.2.5 Version control

This chapter discusses software version control in general [Mas05] and from the perspective of the methods used in this project.

An important part of effective and modern software development is version control. Version control can be considered any maintenance of metadata describing the software's stage of development: in its simplest form, it can be a version number found in a text file or displayed when the program starts. However, simply maintaining a version number is usually not enough; a way to maintain changes made to the software and their dates is also needed, and possibly provide access to previous versions of the source code. A manually maintained list of changes may be sufficient for a small project, but problems arise if more than one person is developing the same software. Usually, the aforementioned problems are solved by using centralized version control. In addition to maintaining version history, it usually allows retrieving the source code of previous versions and supplementing changes with a clear explanation. With the centralized version control offered by modern tools, even large software changes are easy to keep accessible to the entire development team without fear of overlapping or conflicting changes. Solving problems encountered in development is also easier when an accurate history of the changes made to the software code and their dates is available.

In this project, the version control software Subversion [CN01] was used.
The version control system maintains the Iris 440 device's source code tree in several different development branches, including the mainline branch used for development and the release branches that maintain stable source code versions. In addition, the version control system includes a private branch where users can maintain their own code, not intended for publication, and various tests.

There are also other implementations for the purpose, some of which are paid and some are free. Another popular, and also open-source, version control application is CVS (Concurrent Versions System) [Gru86]. In this subsection, in addition to the basic concepts, we will mainly focus on version control with Subversion, as it is primarily used in this project. CVS and Subversion are similar in functionality, and Subversion is generally considered an enhanced version of CVS. Subversion is more advanced than its predecessor, for example, in terms of transaction functionality, so that when saving files, the user can be sure that the saving of changes will not be left incomplete. When using CVS, it may happen that if the saving is interrupted, for example, due to a data transfer error, only some of the desired changes take effect.

Basic concepts

The central concept of version control is a repository, which provides a centralized location for storing the master copy of the project's files in different versions. The repository is a server application that should be located on a dedicated server machine that is reliable, efficient, and has up-to-date security. The importance of data security takes on a special significance, especially when software developers are to be given access to version control over the internet, for example, from their homes. Ordinary files or, for example, a database can be used to store the data.

Although the storage location could theoretically also be used as a traditional file server, its primary purpose is specifically for file versioning, which requires that only software source code be stored there. Other information, such as text-based documents, is also suitable for versioning, as long as they are in plain text. It is not reasonable to version changes to binary data because they are not consistent or understandable to humans. However, binary files can also be stored, which is necessary, for example, for files used in compilation or if the source code for the file is not available.

For modifying the software, the user needs a local working copy made from the main copy, which they can freely modify, compile, and run. By default, the latest version is usually retrieved from the main copy, but the user can also select an older version if desired. The local copy can be a copy of the entire software, or just a single part of it. When developing large software, it is probably easier to take a copy of only the folder or individual file being worked on. Software may contain a large number of directories, but well-designed software enables the development of its individual parts without disturbing other parts, allowing the user to save time and disk space by handling only a specific directory.

Once the user has made the desired changes to their working copy and verified that they compile and preferably also function correctly, they can save the changes to the main copy in the repository. Version control automatically increments the version number upon saving, and the changes in each version are marked visibly. When saving, the user is usually asked for a brief, plain-language explanation of the changes made, which allows the purpose and timing of an individual change to be located more easily later than by examining code-level changes. The expressiveness of version numbers can be improved by naming them with tags.

The master copy in storage can also be branched. This can be useful, for example, when a software developer wants to make extensive changes to the software that may affect the work of other developers. Design Combus Oy's version control uses two development branches, one for the stable production version and the other for the unstable version under development. The branched copy can be developed in parallel with the main branch and merged back into it if necessary. This greatly facilitates situations where a user wants to make uncertain or radical experiments with the software and still use version control.

Subversion

Subversion is a versatile version control software that, in addition to basic features, supports atomic commits, the HTTP protocol, BerkeleyDB storage, and versioning of symbolic links, among other things. Furthermore, copies and branches are lightweight, and binary data storage is also possible.

To implement the steps mentioned in connection with the basic concepts, in the case of Subversion, you can use either commands entered on the command line or dedicated user applications, which make it easy to do the same with a graphical user interface. An example of a good user application is TortoiseSVN for Windows, which provides a straightforward interface for browsing and modifying a Subversion server. Using the command line is also intuitive, and thanks to good instructions, a graphical user application is not necessarily needed.

The commands most often needed by the user are add, checkout, commit, copy, move, merge, and update. The syntax accepted by the command line is in the form of svn command [options] [arguments]. Options are additional options given to the command, which are needed for example to specify the version to be processed. Arguments, on the other hand, are essential inputs for most commands, and they are often inferable from the arguments of basic Unix tools. For example, when moving a folder, the source and destination location of the folder to be copied become the arguments.

A user typically starts modifying software with a checkout command, which retrieves the desired item from the server. The command requires the item's address as input. For example, the command svn checkout svn://server/home/svn/repository/project retrieves the latest version from the main copy by default, but the desired version can also be specified as a setting. Once the user has made the desired changes to the software, they can save them using the commit command. The most common way to use the command is to call the command from the local folder to be saved, or alternatively by giving the folder as an argument. The copy and move commands correspond to the Unix commands of the same name, i.e., they are used to move or copy the desired item. Both require both a source and destination path as arguments. The copy command can be conveniently used to branch parts of the software, and the move command to rename items.

Explanations of the rest of the aforementioned commands and other commands included in Subversion are compiled in Appendix 1.

4 Server application selection and requirements

Every web application needs a server application to support communication with the outside world. For this reason, one important area of the project's preliminary study phase was finding a suitable HTTP server application to serve as the basis for the management application being implemented. One option would have been to implement the server ourselves, but the sheer number of open HTTP servers is large, so finding a suitable ready-made implementation seemed likely. Finding a suitable server application was complicated by the technical constraints and security requirements set by the project's target environment. In addition, the project had certain goals regarding the maintainability of the web application, so the server application's application development interfaces or development environments would be beneficial.

This chapter discusses considering these requirements technically and in terms of design using various criteria. The chapter examines a set of servers selected based on these criteria, from which the most suitable solutions are chosen. The remaining three server applications are examined in more detail, and the server application to be used is selected from among them.

4.1 Categorization criteria

The initial criteria for classifying HTTP server applications were license terms, supported programming languages, process model, possible templating mechanisms, application size, and security. Of these, license terms, process model, and security were considered the most important criteria. Small application size is also particularly important because the application's end-use environment is an embedded system, which means the available memory capacity is limited.

License terms

One important factor in mapping web servers is the license terms they offer. Without the possibility of using a license suitable for the company's business idea, the program is not suitable for use, even if its other features meet the requirements. In this case, a suitable license is open and free of charge, and does not require the publication of the source code of the application created with it. Paid options are also suitable for use if they offer adequate value for their price.

In practice, the license required the exclusion of all GPL-licensed
servers, because the GPL license is "viral" and therefore all
modifications to it must also be released under the GPL. Furthermore, the program code linked to it through interfaces must also be released. [FSF91]

Servers with closed-source code were also initially excluded because it was desired to allow modification of the server code if necessary. This makes the software flexible and allows the specific features of the target environment to be taken into account in the implementation of the server application itself. This facilitates the integration of the application into the existing application environment and can also enable performance improvements or make it easier to migrate the software from one target environment to another.

For our purposes, the best licenses are BSD and MIT licenses, or their derivatives, which allow unrestricted use of the code. You are free to make changes to the code, and you do not need to publish the code used under any specific license; for example, you can publish it as closed source and charge for it if you wish.

Most paid server providers also provide the software source code, and often their license allows for the publication of your own software as closed-source only. Such licenses are also suitable for the project's purposes, as long as the license does not involve any monitoring mechanisms that hinder use and the license price is reasonable in relation to the features offered by the server. Since the server application is intended to be used on a device with an annual production volume of several thousand units, it is desirable that the license costs do not increase sharply depending on the number of target devices of the application.

Supported programming languages

Due to the wide range of servers, it is expected that there are also numerous supported programming languages. Most servers support the CGI or FastCGI interface, which allows the web server to handle requests through programming language-independent environment variables and standard-IO. The problem with standard CGI is its heaviness, as its own process is created to handle each request, which is destroyed at the end of processing. Although FastCGI reduces this problem, it was considered most appropriate for this project to find a method that would directly support the programming language used to write web software. However, since CGI opens up the possibility of deploying application frameworks developed for languages such as PHP, Python, and Ruby, support offered for it was also taken into account.

Since the Config API interface of the Iris 440 device, which is managed using a web application, is implemented with C++ classes, the most straightforward programming language for implementing the management application would be C++ or C. In most cases, the supported implementation language also means the server's own implementation language, so server solutions implemented in other languages, such as Java, were excluded from the mapping. Java was excluded from the language options because there was no desire to install a Java virtual machine in the target environment for performance and integration reasons.

Process model

Based on the points noted in section 2.3, the most suitable process model for the server application in the target environment would be SPED, i.e., a single-process event-driven application. The use of a SPED-type server was expected to facilitate its integration into the existing application environment and would also offer a small saving in terms of the amount of memory used by the server. Multithreaded applications were not considered an impossible option either, but the choice was made to pursue the simplest possible solution, and thus the SPED model.

Using only one single-threaded process can cause server response time issues when concurrent requests arise, even if the project's target environment is not expected to experience heavy request traffic from users. This necessitates careful server implementation, and offering an alternative process model for potential changes in circumstances is also beneficial. Many servers allow the selection of a process model, among other settings, to suit the environment's needs.

Application architecture

Because the administration application was intended to be created with maintainability and reusability in mind, the application architecture used in its implementation had to support some architecture that kept the appearance and content separate. Many PHP application frameworks designed for developing web software would be suitable for this purpose, or for example Ruby on Rails, which is an open application framework developed for the Ruby language that follows the MVC architecture. Application frameworks and architectures were discussed in more detail in section 2.5.

Some servers, such as Klone, Seminole, and GoAhead WebServer, offer their own programming interface for developing web applications, which is considered an advantage in mapping if it is useful in implementing the target application of this project. The usefulness of a server-specific programming interface depends largely on the suitability of the functions it provides for the project. The interface should offer a flexible method for creating generic views and an easy connection to the HTTP interface, so that no attention needs to be paid to the operation of data transfer protocols. Although the interface should be broad enough, an overly versatile interface can be difficult to understand and use. The implementation language of the programming interface is particularly important, as in practice only C and C++ interfaces are usable in this project.

It would also be desirable to offer some kind of page template mechanism that allows dynamically retrieved information to be included in an HTML-type page template. Additionally, handling data belonging to different contexts, such as contact events and requests, should be as simple as possible. This could happen, for example, by storing the variables of different contexts in their own scope, the initialization of which is taken care of by the server program, for example, in connection with a new session. In this way, for example, the handling of different types of variables in the GoAhead web server is very straightforward.

Size

The server's power and memory requirements must be as small as possible because the target device's processing power and memory capacity are limited. When pruning server applications, its size was considered, roughly estimated based on the size of the tar package. This estimation method is inaccurate and was only used as a rough guide. However, for example, Apache and Litespeed servers were easily eliminated based solely on the size of the installation package. The server's modularity helps to limit its memory requirements, allowing only the necessary parts to be included in the server's program code. Thanks to its modularity, for example, the Seminole server presented in subsection 4.3.3 remains lightweight despite its reasonably extensive programming interface.

Security

One important classification criterion was information security, because the SHDSL device is a critical part of the network, which means it must be protected from potential attacks. Ensuring sufficient information security requires at least some authentication method to ensure that only authorized users can access the management application. For this purpose, for example, the authentication schemes offered by the HTTP protocol, which were discussed in subsection 2.4.1, can be used.

Especially for remote connections, support for a secure data transmission encryption protocol is also needed, so support for, for example, SSL or TLS protocols is considered an advantage. These protocols offer the possibility of endpoint authentication using the public key method and data encryption using a symmetric encryption key as described in subsection 2.4.2.

In addition to authentication and encrypted data transfer, the implementation of the management application must consider authorization, i.e., user-specific management features of the program. However, authorization mechanisms are independent of the server application, so they are discussed in chapter five in the context of the design and implementation of the management application.

4.2 Researching potential server solutions

The selection of the web server to be deployed began by searching the internet for promising alternatives. In its simplest form, this was done using the Google search engine, which yielded a wealth of information on web servers with various keywords such as web server, http daemon, and httpd. The search results were weighted towards lightweight server solutions with various qualifiers such as lightweight and embedded. The search criteria were chosen loosely because the aim was to find as many available software options as possible, even if they did not make it to the final selection. Not all selection criteria could be assumed to be clear during the mapping, so we did not want to make any reductions based on uncertain features.

Server applications that met certain pre-criteria were compiled into an Excel file, the columns of which were based on the classification criteria presented in section 4.1. The table of server applications is presented in Appendix 2. Light pre-criteria were used to eliminate some of the simplest servers that only allow the display of static information, as well as heavier servers intended for powerful server machines. The number of concurrent users and the versatility of the information presented enabled by heavier server applications would exceed the purpose of the application to be implemented, as the intention is to create a lightweight management application for only a few concurrent users.

Not all implementations considered were complete servers. There were also a few code libraries that offer a potentially suitable application framework for the project, but require a separate server application for support. On the other hand, an opposite example is the libwebserver library, which aims to provide web server functionality for a traditional application. Using libwebserver is somewhat similar to using a server-specific API, although implementing server features as a library may improve the independence of the web application.

Based on the license, only a small number of servers were eliminated, as most of them were either completely open or offered a paid license. Most of the GPL servers were also feature-poor, meaning that a custom web application would have to be created practically using the CGI interface. Besides the Tntnet server [Mäk04], none of the GPL servers supported an embedded file system, so they would not be of great benefit even when considering an embedded target environment. In an embedded file system, the static files of the web application are compiled into the same binary as the server application. This simplifies the handling of the server and usually reduces the overall size. The prerequisites for using an embedded file system are that it is only needed for reading and that no changes occur in the static data.

Most of the servers also had SSL support, which was considered necessary to ensure sufficient data security. The majority of servers also supported HTTP authentication and almost all used Unix permission management methods for file protection. Less common security methods included URL- and IP-based blocking supported by the Monkey HTTP daemon, and the blocking of suspicious requests and the identification of DoS (Denial of Service) attack attempts supported by the more versatile Abyss Web Server [AT01] and Zeus Web Server [ZT95]. However, the latter two servers were on the borderline of being included in the review because neither has source code available, and Zeus in particular is too heavy for the project's purposes.

The biggest stumbling block for server applications turned out to be support for the SPED process model, as many servers operated as multi-process and most as multi-threaded. Both models were considered poor options because if the web application was to be created using the server's provided API, it would also have to take into account the problems associated with the use of multiple processes or threads.

For the implementation of non-CGI-type web software, the simplest option would clearly be a SPED-type server, as the server integration into the target environment would then be easiest. However, there were only nine SPED-type servers in the group under consideration, some of which were also under the GPL license, so the options were limited. Nevertheless, two servers with promising features were found among these options: Klone and Seminole, which are discussed in section 4.3. The GoAhead WebServer, which is also presented in the same section, was selected for closer examination primarily because of its convincing real-world application examples. In addition, the number of technologies it supports was exceptionally wide for a server application designed for embedded systems. A total of 36 HTTP servers were considered, of which, based on the criteria mentioned in this chapter, there were ultimately seven viable options. These servers are shown in Table 4-1. Due to layout reasons, some of the criteria columns contained in the original table had to be pruned.

Table 4-1. Most promising server applications in terms of features.

AppWeb/Mbedthis [Mbe03] is almost functionally equivalent to GoAhead WebServer, although the latter is slightly more developed. In Lighttpd and thttpd, the most interesting features concerned the integration of PHP-based web software using SAPI (Server Application Programming Interface) modules. However, implementing web software in PHP was considered a secondary solution, as the requirements of application frameworks implemented for it, especially in terms of memory usage, are generally lighter than the requirements of this project. Also, due to the predetermined model of the project's target environment and its interface, the SQL database often used in PHP application frameworks cannot be utilized.

4.3 Most suitable solutions

After the filtering based on the classification criteria presented earlier in the chapter, the original server options were ultimately reduced to three server applications: GoAhead WebServer, Klone, and Seminole. This section outlines the main features and operating principles of these server applications, as well as their advantages and disadvantages for this project. The examples illustrating the applications' operation are simplified and are primarily used to illustrate the core idea of the server, not to thoroughly investigate their operation. This section utilizes the technical documentation of Klone, GoAhead WebServer, and Seminole [KL07], [GA00], [GS06a], and [GS06b].

4.3.1 GoAhead WebServer

GoAhead WebServer is an open-source HTTP server designed for embedded systems, and it has been implemented in many commercial end devices, such as Telewell EA501 ADSL modems. Due to its promising application examples, GoAhead WebServer rose from among numerous other servers to become one of the most viable server options. Its most important advantages are its small memory footprint, supported security methods and the interfaces provided for their use, and its foundation in common technologies such as ASP (Active Server Pages), CGI, and JavaScript. In addition to these technologies, GoAhead offers designers its own programming interface, which facilitates the presentation of dynamic content on web pages. It uses the term GoForms for this concept.

Server settings

GoAhead WebServer does not have an actual configuration file; instead, its settings are located at the beginning of the operating system-specific main program file. For example, for the Linux operating system, the settings are marked in the LINUX/main.c file. The web application directory, security password, listening port, and the number of attempts to find a new listening port are marked there as shown in Figure 4-1.

Figure 4-1. Server settings in the main.c file.

Enabling server security features does not require separate settings, but they are available to the web application through various interfaces. The necessary functions can be found in the um.h and websda.h files, the first of which provides user management for the server and the latter provides user authentication according to the digest scheme.

ASP

Active Server Pages (ASP) is a technology developed by Microsoft that enables dynamic content to be provided on web pages. This is done by embedding scripts in conventional HTML code that generate dynamic content before the page is sent to the user. GoAhead uses a lightweight variation of JavaScript called Ejscript as its scripting language. However, ASP does not bind the designer to a specific scripting language, but it can be freely selected with the language parameter of the script directive. The most commonly used languages are VBScript, JScript, JavaScript, or Perl. The scripts are executed on the server, so the browser does not need to support the scripting language used.

The GoAhead API offers the possibility to utilize C functions through ASP pages. This is achieved using the websAspDefine() call, which requires the name of the C function to be used and its "calling name" from the ASP file as parameters.

GoForms

For this project, GoAhead's most significant feature is GoForms, which allows for the efficient separation of web application logic, or control, from the user-perceived view. Like traditional CGI, GoForms uses environment variables for communication, through which it determines, for example, the address of the connecting party and HTTP request parameters. Unlike CGI, GoForms handles each call in the same process, so the required amount of memory does not increase significantly as the number of calls increases. The desired GoForm can be called with a browser, for example, in the form http://server/goform/myForm?name=Henri&age=22, in which case the processing of the request is directed to the GoForm named myForm. An example of a GoForm is shown in Figure 4-2.

The handler writes the parameters name and age it received to the connection handle wp, and in addition it writes the HTML file header and footer, as well as the response code OK. The example GoForm uses only GoAhead's own API calls, but the received parameters can also be used in a more versatile way, for example, by retrieving additional information stored in a database for the person based on the name and age.

Figure 4-2. Example of a GoForm handler.

The response received from the handler is presented in Figure 4-3. The page header and footer sent by the server can be customized using the websHeader() and websFooter() functions. The application can use the websDone() function on line 7 of the previous figure to return response codes other than the 200 sent here.

Figure 4-3. Response received from the handler.

Despite its technical features, openness, and real-world application examples, GoAhead was not convincing enough in its design to be ultimately adopted in the project. The biggest concern was Ejscript's flexibility in creating sufficiently versatile page templates for different views. Additionally, page template selection would practically have to be implemented either by direct request, redirection, or some kind of include mechanism, each of which is too cumbersome for the purposes of this project. It should be possible to change the page template to be used from a single variable, so that the page template selection is affected not only by the parameters received, but, for example, an error that occurred during data retrieval can cause a specific page template to load. A weakness compared to the other two server applications presented in the section is also the location of the page templates in their own directory, which means that their syntax is only checked at runtime. In this case, many surprising error situations may not be noticed during compilation, but only when the application is running.

4.3.2 Klone

Klone is a web server designed for embedded systems that allows designers to separate the server architecture into distinct parts for display, logic, and data. Klone supports the SPED process model, which facilitates software integration into the target system and lightens the server's memory footprint. The program can also operate in fork or prefork mode if needed, making it suitable for a large number of concurrent requests if performance is not an issue. The fork setting means a mode in which the server can utilize an arbitrary number of child processes, which it starts as new requests arrive. The prefork setting is a mode in which the server creates a fixed number of child processes, set by the user, when it starts. In the project's target environment, the number of connections is assumed to be small, so these settings are not practically needed. However, finding additional options is considered an advantage for the future.

Klone also defines its own programming interface, which facilitates the creation of a web application through its provided functions. The interface includes functions for handling, among other things, header fields, requests, responses, sessions, outputs, inputs, and various variables. These can be easily used to create, for example, simple session management without having to program the entire mechanism yourself.

Klone's special feature is compiling the web application and the server into a single binary, where both the dynamic and static content of the software, as well as the software itself, are compiled into one executable binary. This binary is a kind of embedded file system. The file system is compiled with the operating system's own compiler, achieving efficient software execution and low memory requirements. Klone promises minimum requirements of approximately 140KB of ROM and 70KB of RAM, which are very well suited to the target environment.

In practice, a web application consists of HTML page templates stored in kl1 format, inside of which C-language scripts can be written. The script uses four different blocks: a connection block, a declaration block, a code block, and an output block. Certain parts of the code are placed in each block, which clarifies the structure of the code. However, the intended uses of the blocks are only indicative, so the programmer can take liberties in their use. External functions can also be used in the code by storing them as C files in a separate location and calling them from the kl1 file.

The meaning of different code blocks is as follows:

* Include block: the block allows attaching separate static or dynamic parts from other files to the side. The include block's definitions are enclosed in <%@ and %> brackets.
* Declaration block: inclusions, functions, and global variables needed in the actual code are declared in a separate declaration block. The declaration block's definitions are enclosed in <%! and %> brackets.
* Code block: the code to be executed for each request, a kind of "main-method" of the page, is placed in the code block. In addition to the variables defined in the declaration block and its own variables, the code block can use Klone's predefined functions. The content of the code block is enclosed in <% and %> brackets.
* Output block: the block allows printing the values of variables. The output block's definitions are enclosed in <%= and %> brackets.

For a designer proficient in script languages commonly used on web pages, using HTML page templates and the code blocks they contain is straightforward. This method brings the creation of an embedded system web application one step closer to modern, streamlined web programming.

Server settings

The easiest way to configure Klone server settings is through the kloned.conf file included in the server program, located in the etc folder of the embedded file system. The most common settings to consider are the server type, process model, protocol, port, and web software root directory. The example configuration in Figure 4-4 defines the web software as "oma_www." The process model in this case is prefork, where the program initially creates three kloned processes that handle incoming requests to the server. The application listens on port 80 and searches for pages to display in the www root directory. By configuring multiple web software instances with different names in a similar way, several different websites can be launched with the same application.

Figure 4-4. Klone server configuration file kloned.conf.

The kloned.conf file also contains the server's SSL settings, which define the key files and encryption settings to be used. The configuration file to be used can also be specified when the server is started, in which case the -f switch of the server application is used to point to the configuration file.

Simple request handler

A single page can be composed of several different kl1 files, which can be included using a join block. This join mechanism can be used to create a simple request handler that receives the desired parameters of an HTTP call and selects the page to be sent to the user based on them. In practice, this means that the request parameters are examined using conditional statements, and when a certain condition is met, the corresponding kl1 page template is included.

The following simple kl1 file in figures 4-5 only includes HTTP request processing, but not response generation for the user. In this case, the handler completely transfers the responsibility for output to an included page template, i.e., kl1 file tpl1 or tpl2.

Figure 4-5. Example of a simple dispatcher script that selects the page template to be included.

After connection blocks, the operation can also continue in the dispatch.kl1 file, but with the means offered by Klone, this is unnecessarily cumbersome, because the means for modifying variables in different contexts are not very versatile. Only session variables can be easily assigned name-value pairs using functions that handle the session_t record, while, for example, modifying request-specific settings is not possible in a similar way. Thus, the chain-like forwarding of requests between different kl1 files does not work as desired. Instead, view management should be transferred to another page template and the requests should be retrieved using the request_get_args function and the desired information printed, as presented in the 4-6 tpl1 page template. There, the HTTP parameter named objnm is stored in a variable and printed to the view.

Figure 4-6. A page template that the dispatcher includes when the required conditions are met.

Although Klone's methods for using page templates are straightforward on simple pages, some of its features were not entirely suitable for the project's purposes. As the example illustrates, the dispatcher that performs page template selection is created in Klone's own scripting language instead of being implemented with standard C code. While using the server's own API places certain restrictions on the implementation of a web application, the goal was to keep the server-dependent code portion of the application to be implemented as small as possible. If even the page template selection is done with a server-specific scripting language, the application to be implemented would likely be very dependent on the server used.

Another disturbing aspect of Klone is that it does not precisely define the location of program logic within the web application architecture. While the choice of page template is made in the scripting language clearly alongside the page template after receiving the request parameters, the method for accessing the data to be retrieved can be defined elsewhere, for example, in an included page template. Retrieving data in the dispatcher page template would be a poor option because, in that case, in addition to the page template, the part of the program used to access the data, such as the function call responsible for fetching the objects to be listed, would also have to be executed in the dispatcher page template, which would further complicate it. The latter option, i.e., retrieving data from an included page template, would be poor because views might not be one-to-one with the data retrieval method, and several different views might retrieve the data in the same way. Neither approach follows the MVC model, which was determined to be a suitable web architecture for the project in section 2.5.

The third, and in this case perhaps the most sensible, way would be to create a generic function in C for retrieving data, which would retrieve the information needed by the views in a uniform way without the logic responsible for generating the view needing to know the data it is retrieving or how to retrieve it. In the management application to be implemented, in addition to data retrieval, separate functions would also be needed for adding, modifying, and other operations on management objects.

4.3.3 Seminole

Of the three most promising server applications, Seminole proved to be the most suitable.

This subsection outlines the technology and basic principles of the Seminole server. For this project, Seminole's most important feature is the isolation of the view from the program logic using HTML templates, so the simplified example program in the subsection utilizes this technique.

Seminole is an HTTP server designed for embedded systems, with advantages including a very small size, a versatile programming interface, and a modular structure.

Seminole's high-level programming interface isolates the programmer from low-level protocol details but allows for their modification if necessary.

Common application areas for Seminole include web interfaces for embedded systems, remote procedure call handling, adaptive auxiliary functions, and traditional systems that need to enable data transmission via the HTTP protocol. Seminole's implementation language is C++, which enables a loosely coupled modular structure, so that unused features do not cause extra load. Custom request handlers can be created for servers designed for different purposes, allowing the server to have exactly the functions it needs. Seminole includes some standard handlers, for example, for redirection and file serving. You can create your own handler for your application as needed. With numerous classes, you can utilize, among other things, page templates, request and response handling, SSL encryption and authentication, and session management in your web application.

Like Klon, Seminole also offers the possibility to compile the entire server directory path into a single binary, which makes running the server fast and exposes many errors during the compilation phase. In addition, the programmer can fine-tune the application with numerous compile-time features. When the server and the web application implemented for it are in a single file, moving it from one directory or even from one computer to another is effortless. Of course, when the environment changes, you must ensure the compatibility of the binary and system interfaces, and the availability of runtime-loaded libraries.

One of Seminole's strengths is its portability to different platforms. Its platform-specific code is isolated to a so-called portability layer, which allows it to offer ready-made support for many platforms, such as POSIX (Solaris, Linux, BSD, etc.), Win32, VxWorks, uC/OS2, and eCos. Although there is no apparent need for platforms other than Linux within the scope of this project, portability provides flexibility for the future and also indicates careful software design. In addition, Seminole's completely open source code allows the designer to take into account the specific characteristics of the target environment.

Server settings

Like GoAhead WebServer, Seminole also does not include a dedicated file for server settings, but rather they are usually defined using the c_servername and c_serverport variables in the globals.cpp file of the web application being used, which represent the server's host name and listening port.

Server security settings can be managed in the ports/Seminole file, which contains settings related to compiling the server. The file can be used to set the INC_BASIC_AUTH and INC_DIGEST_AUTH variables, which determine whether the possibility of using basic or digest authentication schemes is included in the server code. The actual activation of authentication is done in the web application, where an Authenticator object is created for the HttpdHandler class (i.e., the request handler), and allowed usernames and passwords, as well as the realm covered by the handler, are defined for it.

In addition to authentication, Seminole also enables the use of SSL encryption, which can be enabled by setting the INC_SSL variable in the ports/Seminole file and modifying the main.cpp file of the web application to call the Httpd::Start() method with parameters corresponding to the desired SSL settings. Furthermore, the INC_MULTIPLE_TRANSPORTS variable in the ports/Seminole file must be set for the server to be able to maintain multiple socket types. The web application implemented in the project used the SSL settings presented in Figure 4-7.

Figure 4-7. SSL parameters given to the Httpd::Start() method.

The parameters specify the key file to use as web.pem, which contains the PKI certificate to be used, i.e., the server's public and private key. The socket type is set to SSL so that the transport selector knows to use the SSL protocol instead of the standard TCP protocol. The random strings needed for encryption are obtained from the Linux random number generator.

Page templates

The most important aspect that distinguished Seminole from the other HTTP servers examined was not its small size, embedded file system, or security features, but rather the methods it offered for creating views. Seminole's view types can be implemented as HTML page templates, where the data to be presented is defined using various loop, conditional, and evaluation structures. This method is useful for handling dynamic information in a way that makes the implemented web software reasonably maintainable.

Templates help with maintainability because they allow modifying the appearance and layout of pages without requiring changes to the application logic. Furthermore, a template is often independent of its data content, so the amount of work required to implement views does not increase as the data content grows. In Seminaari, the syntax of template structures is checked on the server during compilation, avoiding unnecessary runtime errors.

Displaying information on the page template is done using the `HttpdFSTemplateShell::Execute()` method. This method requires, as parameters, the request state and the desired information content to be input, which in Seminole terms is called a symbol table. The symbol table contains the data to be presented in the view, which can be defined in the page template to be presented as desired. A special case of a symbol table is a symbol map, which facilitates the presentation of data stored in records.

When the HttpdFSTemplateShell::Execute() method is called, it handles all output-related pre-processing and processes the desired page template, which lays out the content of the symbol map defined for Execute() in the view. Figure 4.8 illustrates how to store the desired information in the symbol map and start the view processing.

Figure 4-8. Setting user account information and the page layout in the demo.cpp file.

The first argument of the Execute() method is the record state, which contains information about the received HTTP request and the file determined based on it. The page template to be used is defined by the state.mpFilePath variable set on line 17. This variable provides the freedom to use the symbol map's data content on multiple different page templates. This is especially useful when you want to offer the user the ability to view data in several different formats, such as an HTML page or an RSS feed. Figure 4-9 presents a page template that shows how the values to be displayed are defined using the eval command. The command tells the name of the variable to be displayed on the page template. The variable names in the page templates are defined in the structure of the symbol map (user_account_map).

Figure 4-9. A page template that determines how information is displayed.

Other possible directives include the loop command for loops and the if and ifnot commands for conditional statements. Attributes can also be set for each command, based on which the handler responsible for executing the command can implement various additional checks. The page template closely resembles the final HTML, so its implementation in terms of composition and appearance depends as little as possible on the underlying controller and model. From the page template's perspective, the method of obtaining information or its original location is irrelevant; only aspects related to its presentation matter.

5 Design and Implementation

The first step in designing the web administration application was designing the application's graphical user interface. This design phase was first because the application's customer requirements were quite well known at the beginning of the project, and they would not depend much on the technical implementation of the project. It made sense to design the user interface based on the application's functional requirements before the technical design. The functional requirements are based on the general use cases for managing the Iris 440 device, which are determined based on the features of the CLI management application and the device's user manual.

After the user interface design, the next step was to design and implement the application's classes. The methods and data structures required by the classes also had to be designed. The class architecture design was based on pre-designed views, from which the necessary classes were deduced. The methods and data structures required by the classes were created according to the model located below the application engine and the requirements of the final views. This chapter also discusses the integration of the management application into the device's application environment. At the end of the chapter, the different stages of response generation are discussed from the perspective of a web application.

5.1 User Interface

The actual, concrete views of the user interface were called views, and their different classes were called view types. Concrete views corresponding to different management objects can be classified according to their type, resulting in the generic view types required by the management application. With such a classification, the final implementation of the application could be based on the designed view types, reducing the required workload. When each view is designed based on a specific view type, adding a new view of the same type in the management application only requires defining the view's parameters. The view's parameters include, for example, the attributes of the management object to be hidden or references to other attributes.

In designing view types, the focus was primarily on defining the types of attributes they contain (predefined key fkattr, key kattr, or regular attribute nattr), functions, and the layout of different elements. The view type attributes were marked with an abbreviation corresponding to the attribute type, and the functions with boxes representing buttons, containing text describing the function. Other definable elements include, for example, the view title or the links it contains to other views.

The user interface design plan was implemented as a PowerPoint presentation, with each view on its own slide. The design aimed to match the final views content-wise as closely as possible, although the final appearance would be determined by the use of description and scripting languages (HTML, CSS, and JavaScript). Technical aspects, such as the format of input fields and the use of links, were also addressed as comprehensively as possible.

5.1.1 Design Principles

The management application's user interface design was based on the use cases indicated by the CLI management application and the implementation of the features they required. Use cases are generally considered a good solution for mapping user requirements, as they effectively demonstrate the features required of the program from the user's perspective [HaMä98, p. 135]. The graphical user interface should be able to provide at least the same viewing and modification capabilities as the CLI management application. This mainly involves managing and appropriately presenting bridges and their ports, physical interfaces and their VLAN (Virtual Local Area Network) settings, device network settings, management settings, and SNMP settings. The same use cases were later used in the user interface usability test, which is discussed in subsection 5.1.5. The elucidation of use cases also utilized the configuration options presented in the user manual prepared for the Iris 440 device and various wishes expressed for the user interface.

The primary design basis was chosen because the reception of the CLI by customers has been mostly good, and because it comprehensively presents the features required for Iris 440 to be modified or displayed. Nevertheless, during the design process, it must be remembered that CLI and web interfaces are functionally different, so a solution that has proven good in the CLI may not work in a graphical interface. Therefore, it is wise to avoid slavishly copying CLI features and to take certain liberties in the design of the web interface.

In addition to this, the genericity of the application was considered a general design principle, which would allow for easy modification and extensibility. This is reflected in the design of the user interface as the genericity of view types. View types can be parameterized, and by modifying the parameters, concrete views required by different object types can be implemented. In simplified terms, the user interface can be divided into four different view types: object addition, modification, display, and object listing. These four main types were aimed to be used as comprehensively as possible in creating the entire management application, although other view types also had to be implemented for more specialized views and exceptional cases.

In addition to the actual design criteria, the software and its user interface were designed to be implemented incrementally, initially implementing only the necessary basic functionality. In mapping the view types, the goal was to find the minimum number of types with which the management application could be implemented without significantly compromising usability. This allows for a quick implementation of a framework for the software, to which features can be added as needed [HuTh00, p. 48]. However, minimizing the number of view types should not reduce the device's management capabilities. Once the basic functionality of the program is complete, views and functions that improve usability and functionality could easily be added. Nevertheless, it is clear that the number of view types will increase during the project, and it is advisable to collect a list of features and views to be implemented later as the work progresses.

The initial minimum set of view types was a compromise between consistency across views and sufficient usability of the application. As consistency increases, consideration of the specific characteristics of different object types decreases, easily making the views rigid and inconsistent with their intended use. However, this was initially allowed because increasing the number of view types also increases the amount of work required to implement them.

5.1.2 View design

In the initial versions of the user interface design, the aim was to create a comprehensive picture of the different views and their associated characteristics. Although a generic approach to designing views was kept in mind from the beginning, there was no consistent effort to immediately fit views into some pre-planned view types. Instead, the intention was to design the view types only based on the required views, allowing the characteristics of different views to be better taken into account. According to these principles, the first versions of the user interface design included significantly more view types than were ultimately decided upon. The number of view types could later be reduced by combining similar view types.

One problem that arose in the design of the graphical user interface was a situation where a view contained editable attributes and transitions to other views. In such a case, it might be unclear to the user whether the data entered on the page would disappear upon transition or whether it would remain in memory awaiting the user's return. For this reason, for example, a form view should not simultaneously contain transitions to other views, except for saving and canceling. By limiting the transition possibilities of such a view to these two, the problem that would arise if the page were transitioned elsewhere after a form input field has been modified is avoided. In practice, this means a separate view for displaying and manipulating the attributes of a management object. In addition, the manipulation of attributes was further divided into separate add and edit views. However, transitioning using the navigation menu buttons during editing is still possible, but then the user can be assumed to understand the loss of form data.

One natural difference between a CLI and a web user interface is that, while editing and displaying an object occur through separate commands in a command-line interface, these two actions can be partially combined in a graphical user interface. The implementation of the bridge editing view presented in Appendix 3 requires careful consideration of which object attributes are editable and which are only displayable. In the figure, the editable attributes of the object are marked in italics. In the bridge editing view, its key attribute, i.e., the name, cannot be editable because changing it would also change the bridge being edited. Different view classes can be categorized into four main classes: add, edit, list, and show. There are numerous variations of these main classes, and some special views, such as static views, are also needed.

View types

The add view type is a view type for adding a management object, which includes save and cancel buttons. The view title contains the name of the object type being added. The attributes in the view are divided into predefined key attributes (fkattr), which are not editable by the user, as well as key attributes (kattr) and regular attributes (nattr), which the user can select themselves. An example of this type of view is the addition of a bridge in appendix 4.

The edit view type is used to modify an existing management object and, like the Add view, includes save and cancel buttons. The view title contains the name of the object type being edited. The attributes in the view are divided into key attributes (kattr), which are not user-modifiable, and regular attributes (nattr), which the user can select themselves. An example of this type of view is bridge editing in Appendix 3.

The list_cut view type is used for listing management objects. A characteristic of the list view is that the number of objects is not explicitly limited, and, as the view type name suggests, the number of displayed attributes is truncated. This type includes buttons for displaying, deleting, and adding objects, and its title includes the list's filtering criteria and the object type name. An example of this type of view is the listing of bridges in Appendix 5.

The list_closed view type is a variation of the list view, characterized by a predetermined number of objects. This type includes buttons for displaying, deleting, and adding objects, and its title includes the list's filtering criteria and the object type name. An example of this type of view is the listing of Ethernet interfaces in Appendix 6. The Iris 440 device contains four Ethernet interfaces, so these interfaces are presented in the view regardless of whether they are enabled or not. An enabled interface can be viewed with the show button and removed with the disable button. A disabled interface can be activated with the enable button.

The list_reset view type is a variation of the list view, characterized by a reset button that allows object resetting. Additionally, the type includes a button for displaying the object, and its title contains the list's filtering criteria and the object type name. An example of this type of view is the listing of interface counters in Appendix 7.

The image_download view type is used for downloading a new disk image to the device. This type was created for a view of the same name, which includes an input field for the disk image address and a button to start the download. The view is presented in Appendix 8.

The image_install view type is used to install a new disk image onto the device's flash memory. This type is created for the view of the same name, which includes information about the retrieved disk image and a button to start the installation and destroy the disk image. The view is presented in appendix 9.

The progress view type has been created for a view of the same name, which is used to indicate to the user that an action, such as copying or installing a disk image, is in progress. An example of the type is disk image copying in Appendix 10.

The static_config_menu view type has been created for a view of the same name, which is a menu view for handling the configuration file. It includes buttons for saving and deleting the configuration file, as well as loading factory settings and restarting. The view is presented in Appendix 11.

The static_confirm view type has been created for a view of the same name, which is intended to confirm a restart. It includes buttons to confirm and cancel the action. The view is presented in attachment 12.

The static_flash_complete has been created for a view of the same name, which is intended to notify the user when the disk image installation is complete. It includes completion text and a reboot button. The view is presented in attachment 13.

The static_front view type is intended for the front page, which may display, for example, the Design Combus logo or information about the management application or device. The front page view is presented in appendix 14.

In the initial UI design plans, a back button was used in several views to facilitate navigation between views. For example, a view displaying object attributes that does not contain any actions (view type show_noedit, for example, show SNMP community in attachment 15) would be more user-friendly based on usability testing if it provided an easily discernible way to exit the view. However, the button was clearly perceived as an incremental feature, so it could be added to the necessary views later. Furthermore, it would be similar to the browser's own back button, which may not need to be replaced. Also, the back button is not as easy to implement as, for example, cancel used in add or edit views, because unlike the cancel case, the previous view is not unambiguous. In the add view, cancel always takes you to the listing view of that object, and in the edit view, it takes you to the object's display view.

Indicating the accessed object

When an object is shown to the user, or it is being added or modified, there needs to be a way to indicate to the user which object it is. This can be done, for example, by displaying the values of the object's key attributes in the title, or by listing all the information normally in a vertical list. The clarity of different methods is case-specific, as indicating some object types is clearer in the title and others in the form of an attribute list. However, due to the uniformity of view types, such versatile consideration of the specific characteristics of object types was not appropriate, so an attempt was made to find a method that works best with all object types.

Even though the object attributes appearing in the heading often clearly tell the user what object is being handled, the heading can become unclear if the number of attributes it contains is large. Some object types may contain more key attributes than other attributes, in which case the number of attributes presented in the heading would be disproportionately large compared to the total number of attributes. On the other hand, converting the heading into a human-readable format can also be difficult compared to listing attribute name-value pairs vertically. The use of a heading to indicate attributes would be supported by the fact that the heading is used in many views anyway to identify the object based on the object type or list filter attributes.

The chosen solution was to list attributes as name-value pairs to avoid having to worry about the number of key attributes relative to other attributes during design. In addition, the amount of information presented increases, because both the name and the value of the attributes are visible, instead of just the value.

5.1.3 The navigation nenu

A navigation menu was implemented on the page for navigating between views of different administrative objects, allowing the user to select the desired administrative object. Almost all of the menu buttons open a list of objects of the corresponding type, with a few exceptions that open the display of a single object. It should be noted that the navigation menu is not comprehensive in terms of navigating between views; some navigation takes place using buttons within the views themselves and the browser's back button.

The goal was to make the menu as independent as possible, which means that the menu's current state would be static in relation to the current view, and the functions performed by the different buttons would always be the same. For this reason, some object listing buttons, such as VLAN interfaces or ports belonging to a bridge, are links within the views themselves instead of being accessible directly from the navigation menu. This eliminates the need to maintain referential integrity between different objects in the navigation menu; it is sufficient for the objects in the views to know their references to other objects. Determining the state of the menu only requires the current object type, based on which the branch of the menu being accessed can be estimated. For example, Figure 15 in Appendix 15 shows an SNMP community saved on the device, based on which the menu is assigned a state where the SNMP branch is open.

During the planning phase, it was known that the amount of code required to implement the navigation menu would depend on its implementation method, and more specifically, the degree to which it was bound to the project's target environment. In its simplest form, the menu could be hardcoded, as practically every necessary menu button and its parameters are known in advance. In this case, each state of the menu could have a corresponding identifier, which would then be included as a parameter of the view when the view is being constructed. Thus, each view would correspond to a unique menu state, which could practically be a static file containing the HTML description of the menu to be included in each view.

Another option would be to make the menu smarter, implementing product-specific data and logic that utilizes the current state of the view. The menu could retrieve the available object types and use reference markers to create a navigation menu corresponding to the current target device. If the menu knew the current view, all management objects could be accessed from the menu. For example, the list of VLAN configurations for a specific Ethernet connection could be displayed from the menu below the button for that connection.

However, a simpler, explicit navigation menu was chosen due to the urgent schedule and the known target environment. If the characteristics of the devices to be managed varied physically, creating some kind of dynamic navigation menu might be a more realistic option. In practice, the HTML templates representing the different states of the navigation menu were stored in their own files, from which the control selects the correct file based on the management object related to the view. Thus, the menu state corresponding to the view is added to the view during program execution. The basic menu state was used by default to ensure that no view was accidentally left without a menu. The downside of the solution is its presumably poor compatibility with future product projects, but on the other hand, the amount of code that needs to be rewritten is not large.

5.1.4 Button functionality

Each view type includes specific functions that the user can perform by clicking the button corresponding to the function. In the final application, the buttons are submit inputs of forms that send the form parameters using the desired HTTP method. Each function is associated with the parameters it requires, the format of which is the same for each function. The parameters consist of the function type, the object type targeted by the function, and additional parameters. The final content of the parameters depends on the data contained in the current view, based on which the request to be sent with the button is formed. The parameter information can come from the model, user-filled input fields, or be fixed in the page template.

Different action types include Show, List, Openadd, Openedit, Add, Update, Delete, Download, Flash, Saveconfig, and Removeconfig. These action types are mapped in the server application to corresponding action objects that process the request using the received parameters. From the controller's perspective, the received parameters are considered equivalent, and their final semantics depend on the action being called. The controller directs the parameters obtained as a result of parsing the HTTP request to the object corresponding to the action and object type. This object knows the meaning of the parameters it receives and processes them as required. The controller and action classes are discussed in more detail in section 5.2.

Next, we will go through the role of different functions in the application and the parameters they require:
* The show function opens a view displaying the attributes of a specific object. The function requires the object's unique identifier as a parameter, i.e., its type and key attributes.
* The list function opens a list of objects retrieved based on specific filter attributes. The function requires the values of the filter attributes as parameters.
* The openadd function opens an add view for a specific object. The function requires the object type and predefined key attributes that the user can no longer change in the view as parameters.
* The openedit function opens an edit view for a specific object. The function requires the object type and key attributes as parameters.
* The add function saves a new object with the given values. The function requires the object type, key attributes, and other specified attributes as parameters.
* The update function updates a specific, existing object with the desired values. The function requires the object type, its key attributes, and other specified attributes as parameters.
* The delete function deletes a specific object. The function requires the object type and key attributes of the object to be deleted as parameters.
* The download function starts downloading the device's disk image and requires the URL of the file to be downloaded as a parameter.
* The flash function starts installing a specific disk image on the device and requires the object type and key attributes as parameters.
* The saveconfig function saves the management application's current settings and does not require any parameters.
* The removeconfig function removes the current settings and also does not require any parameters.

The buttons have been composed in the views in such a way that the user perceives certain functions as similar based on their proximity [Kal95, p. 156]. For example, in the list_cut view of appendix 5, this is reflected in the fact that the new button is separate from the adjacent show and delete buttons. This is because the new button creates a new object, while the show and delete buttons manipulate a specific, already existing object.

The post-execution view cannot be described unambiguously because it depends on the parameters provided and may change depending on the course of processing. For example, certain object types use a different listing view than others, and if processing results in an error situation, a different view may be given than if the operation succeeds.

5.1.5 Usability testing

As the user interface takes its final form, its suitability for end-use should be tested with a usability test. Even a simple cardboard model usability test can reveal many usability problems that are not considered during the design phase [HuTh00, p. 53]. Because only two people were practically involved in the ideation and design of the user interface, it can easily retain features that too closely reflect the designers' personal view of a good user interface. In addition, a designer's full-time focus on their work can prevent them from seeing simple errors or omissions. An example of this was a view that clearly showed the attributes of the wrong control object, and instead of the designer, the test person immediately noticed this during the usability test.

Usability testing seeks to identify program defects and shortcomings that cause users headaches by observing user behavior with the program [OvRa98, p. 13]. It is advisable to select the most frequently used functions for testing, as the benefits gained from usability improvements in these functions are the greatest [Kuu03, p. 72]. Therefore, the testing was conducted based on the most common use cases, which are assumed to be essential for device setup. Some use cases were written down for testing, and problems and observations that occurred during the testing were recorded. The use cases were not followed rigidly; the test subject was free to invent use cases themselves if they wished. This freedom was given because the test subject was an expert on the device and knew what settings the device would need to have in final use. The test subject's own use cases and the observations made in them were also recorded in a similar manner.

In practice, testing took place by projecting a PowerPoint-based user interface design onto a screen, and the test supervisor told the test subject what to do. Based on this, the test subject verbalized which functions of the views they would use and what they would enter into the forms. The test supervisor navigated through the views according to where the person's current action would take them. In addition to any problems encountered, the supervisor also noted down any positive aspects mentioned by the test subject and other side comments. Video recording could also have been used during testing, allowing the test situation to be reviewed in more detail afterwards. However, this was considered difficult to implement due to poor technical capabilities and unnecessary because the test supervisor took careful notes during the test.

Summary of remarks

The following observations are problematic areas that emerged during testing, in order of appearance. Additionally, briefly identified suggestions for improvement are potentially included.

* In the Ethernet interface show view, the test subject did not immediately grasp the meaning of the "mode" attribute, which should be clarified so that the user understands it means the link speed and mode (e.g., 100Mbps full-duplex).
* The "new" button leading to the add view is ambiguous in meaning. An "add" button would, according to the test subject, be more consistent with the name of the view.
* There was confusion about the view that appears after the save function in the add view. This confusion was partly due to the early stage of planning.
* The indication of incorrect values in the add and edit views has not been clarified. Will the browser load its own error-indicating page, or will they be displayed on the page where the input was given?
* How does the user know the required format for the input in the add view? The default value in the text field helps, but it is probably necessary to add some help text next to the text field.
* Should at least some of the views include a back button? The button might improve usability because the test subject is not used to using the browser's back button when using CGI-type web applications.
* When a new unnumbered or VLAN-numbered port is added to the bridge, the default value of 1 for the line may cause confusion in connection with the Ethernet port.
* When a user adds a new management IP, will they be able to remember the name they gave to the bridge to which they want to add the IP? A valid solution would be a drop-down menu of created bridges.
* Should it be possible to configure the management IP for each port separately? The test subject believes that enabling this is necessary, even though it might confuse a new user.
* The possibility to delete a saved configuration must be added. This feature was completely overlooked by the designer because they had not noticed this feature in the CLI.
* In the add view of the SNMP community, should the default network be displayed as 0.0.0.0/0 or "default"? "Default" is probably more descriptive, but this can only be considered in the implementation phase when more information is available about the content generation methods.
* What happens when an empty value is entered in the form in the add view? The situation differs from the CLI in that it is not possible to give an empty value to an attribute.
* Should the network interface show view have a link to the packet counter for that interface? The test subject thinks that finding the counter could be easier through the interface, but following the design principles, a method consistent with the CLI will initially be used to display the counters through the statistics page.
* Show view links could include the word "configure" (for example, "configure VLAN ports" instead of just "VLAN ports" to configure the VLAN ports of the interface) so that the user understands that they can not only view but also manipulate the object in question.

In analyzing the problems encountered during testing, it was necessary to consider that the person being tested is an expert in the Iris 440 device and, in particular, its CLI management application, as previously mentioned. Consequently, no significant positive conclusions can be drawn from the test, especially regarding the informativeness of the user interface and the presentation of management objects.

Often, usability testing raises more questions than answers [Kuu03, p. 80]. The test revealed many things that had been overlooked during the design phase. Simple solutions could be quickly found for many of them, but several issues that arose were left out of the user interface design. One example of such an issue is the feedback after saving in the add view if the value of a text field is empty. For this, as for many other similar issues, various solutions easily come to mind, such as saving a default value given in the object definition on behalf of the user or an error message. However, the decision on the final solution should probably be made only during the implementation phase, when the rest of the application's structure and solution options are clearer. Nevertheless, the issues that have arisen should be documented so that they can be recalled later in the project.

5.2 Web-application architecture

The web management application implemented in the project consists of two main parts, the first of which is the core of the management application implemented by the Seminole HTTP server provided by GladeSoft Inc. It provides the project's web software with an interface for receiving HTTP requests and sending responses. It also includes an application framework that can be used via a programming interface to utilize various page templates in the implementation of views. Seminole was discussed in more detail in subsection 4.3.3, which illustrated its operation with an example.

This section discusses the second main part of the management application, which is the web application being worked on. It includes its own data structures and classes for the model, i.e., the use of Config Manager, which is used through the Config API interface. The data structures and classes are used to modify the data obtained from the model into a format understood by HTML page templates.

5.2.1 Data Structures

The first data structure a web administration application's server-side HTTP request encounters is an STL (Standard Template Library) map of type `map<string, string>` used to store the request's parameters. This type defines a map where the name and value of each data element are strings. The HTTP request parameters are stored in this map, and the requested action and object type name are extracted into their own strings. The remaining part of the request stored in the map is then passed to the action object corresponding to the action and object type, where it is processed.

The advantages of using the STL map as a data structure are automatic memory management and flexible usage options, of which the most used in this application are key-based search and iteration. Automatic memory management reduces the time required for programming work and helps to reduce the number of careless errors in the code.

The most important data structures of the web management application are the management objects and their attributes, which are obtained from the model via the application's Config API. A management object refers to something manageable in an Iris 440 device, for example, a bridge or a network interface. Attributes are management features contained within a management object, which can be viewed or modified where possible. The object obtained from the Config API is an object that contains a set of public and private methods and variables. From the web management application's point of view, the most important feature of the object is the attributes it contains, of which it owns an arbitrary number stored in an STL map. The map type is map<const attr *, const value *>, meaning it contains pointers to the attribute name and value. The map is a private variable of the object, and the values of the attributes it contains can be read, for example, with the method construct_attr_value_clone(const attr *at), which returns the value of the attribute given as a parameter. In addition to the management object, the web management application also needs access to other information required to create the view in the manner required by the object type and the action to be performed on it. These requirements mainly concern the object's attributes, the presentation of which must comply with the visibility settings and modification possibilities defined for the attribute.

For these requirements, each object obtained from the model is stored in a ViewData record, as presented in Figure 5-1. The structure includes the object's pointer and the necessary flag maps to indicate whether each attribute of the object is a pre-defined key (fkflags), contains an erroneous value (errflags), is hidden (hdnflags), or is displayed (shwflags). In the management object's modification form, each attribute can be set as a pre-defined key, i.e., non-editable, or as containing an erroneous value, in which case it is indicated as erroneous. Attributes allowed to be saved in the form are defined by designating them as displayable. When listing and displaying objects, attributes can be set as hidden.

Figure 5-1. ViewData record for storing the management object.

If you want to store more than one object, such as is necessary when creating a list view, the objects and their corresponding ViewData structures are stored in an STL list of type list<ViewData>.

Using the values found in the ViewData record, the view can be made to determine solutions for displaying, locking, or indicating errors for each attribute. The hdnflags and shwflags maps contained in the ViewData structure may seem contradictory, but the existence of both is explained by the different ways of thinking about how to display attributes for different types of actions. For example, in a saving operation, it is primarily important to save only the attributes defined as enterable in the form and discard the rest. When displaying object attributes, this is not necessary; instead, the desire is to define attributes to be hidden because unwanted attribute display does not pose a threat to the device's security.

One noteworthy data structure is the CActionKey class used in the application, as depicted in Figure 5-2. It is used to identify different action categories and contains information about the action's user mode (m_user), action name (m_action_name), and object type (m_objnm), as well as the class constructor and destructor, a comparison method, and various get methods. The CActionKey class is used for mapping, storing, and searching action objects in the application.

Figure 5-2. The CActionKey class for identifying actions.

Numerous action objects are stored in their own STL map, of type map<CActionKey, CAction>, meaning the key for actions is a CActionKey object created based on its properties. The map allows easy retrieval of the action corresponding to a received HTTP request, to which the execution of the request is then directed.

5.2.2 Classes and methods

The web management application is divided into several classes, which share the server's main task: receiving, processing, and sending HTTP requests. The class diagram is presented in Figure 5-3. There are five important main classes for the web management application, from which some additional classes are inherited. In addition, Seminole, which acts as an HTTP server, contains its own classes, most of which do not need to be discussed within the scope of this work. The web application implemented in the project connects to the core HTTP server through the CWebHandler and CSymbolTable classes, which inherit from the HttpdFileHandler and HttpdSymbolTable classes.

Additionally, HTTP authentication is handled by the HttpAuthentication class provided by Seminole. These three classes are practically the only parts of the program that use the HTTP server's own programming interface, so the other classes of the program can be considered independent of the server application.

Figure 5-3. Class diagram of the web management application.

Class collaboration is based on the MVC application architecture, which was discussed in section 2.5. Most of the classes represent the C part of the architecture, i.e., the controller. CWebHandler handles the application's access to the outside world, which means parsing the parameters of HTTP requests and initiating the response formatting and sending phase. Communication with the model is handled by action classes inherited from the CAction base class via the Config API interface. Information received from the model is ultimately passed to the CSymbolTable class, which is responsible for providing the information to the view in a format that templates understand.

CWebHandler

The CWebHandler class, inherited from the HttpdFileHandler class, is a request handler whose objects are created by the server to respond to HTTP requests arriving with a specific prefix. The name of the HttpdFileHandler class comes from the fact that it allows the server to be used as a file server. The class constructor requires the prefix corresponding to the handler, the file system type to be used, and the default directory as input. A CWebHandler object is created in the server's main program, and its lifetime is practically the duration of the application. Multiple different handlers can be created, each responsible for handling HTTP requests arriving with a specific prefix, for example, a request to http://www.palvelin.com/webForm1 is routed to the handler built with the prefix webForm1, and the address http://www.palvelin.com/webForm2 to the handler built with the prefix webForm2. In the web management application, the CWebHandler handler is used as the default handler for requests arriving without a prefix.

The handling of a CWebHandler object-oriented request occurs in several stages, the first of which is the parsing and directing of the request to the controller in the TranslateUri() method. The requested action and object type name are stored in their own strings, and the rest of the request is stored in an STL map, as described in subsection 5.2.1. Based on this information, a request-specific CRequest object is created and passed on through the controller interface.

User authentication also occurs in the TranslateUri() method, utilizing the HttpdAuthenticator class, which uses HTTP authentication schemes. This class defines the realm, for which users must authenticate using a username and password obtained from the device's passwd module. Different user roles include admin, monitor, and engineer, each with its own defined allowed functions.

Once the controller has processed the request, the CWebHandler object retrieves the symbol table and page template filename specified by the controller from the corresponding CRequest object, storing their pointers in its private variables and continuing execution in the SendFile() method. The SendFile() method calls the static HttpdFSTemplateShell::Execute() method, passing it the request's state record and the symbol table.

The Execute() method generates a unique view based on the provided parameters and sends it to the user who made the request. The method is similar to the example presented in subsection 4.3.3, except that instead of the HttpdSymbolMap class, it uses a class inherited from the HttpdSymbolTable class as a symbol table, where each command originating from the page template is defined individually. The purpose of the HttpdSymbolMap class is to facilitate the presentation of data stored in C language structures in a view using predefined command handlers, but with the data structures used in this project, it is easier to write the command handlers themselves. Command handlers are callbacks used by page templates, whose task is to print the requested variables in the view.

CRequest

The CRequest class is intended as a storage location for request-specific information, and thus it also contains state information maintained during the request. State information includes, for example, information about the last operation performed for the request and the data and page template assigned to the request. During execution, a CRequest object is created in the request handler, where it is initialized with the action parsed from the HTTP request, the object type name, and the remainder of the request.

CControl

The CControl class contains the core of the application's control part. A handler calls a CControl object it knows and passes it a request-specific CRequest object. CControl does not know what type of operation it is dealing with at any given time; its only task is to deliver the parameters it receives to the correct location, i.e., to the action object it receives from the CDispatcher object. In particular, the CControl object knows nothing about the parameters contained in the latter part of the request or their meaning; instead, they are fed to the action object received from the CDispatcher object as is. The CControl object also sets the navigation menu state corresponding to the object type being processed for the request. Although the role of the CControl object in processing the request and forming the response is quite small, it has an important task in conveying the request to the CDispatcher object and in successfully executing the action received from it.

A CControl object is created when the server starts and destroyed when it shuts down. It provides the CWebHandler class, which processes requests, with a simple interface through its ProcessRequest() method, which requires a CRequest object initialized with the request as input.

CDispatcher

The CDispatcher class is intended for managing various action classes. Similar to a CWebHandler object, a CDispatcher object is created when the server starts and destroyed only when it shuts down. The CDispatcher::Init() method creates an action definition object (CActionDefinitions) and requests it to create the actions defined for it. When a controller calls the CDispatcher::GetAction() method, it iterates through the actions provided by the definition object and returns to the controller the pointer to the action object that corresponds to the user mode, action type, and object type of the received request. If more than one action is found based on the CActionKey object defined by these three variables, the correct action can be identified based on the rest of the request. If the object type is not defined, a pointer to a default action defined for a specific user mode and action type can be returned. If no action is found, a pointer to the CAction base class object, which does nothing but can be treated like any other action object, is returned.

CActionDefinitions

The CActionDefinitions class defines all the actions needed by the web management application, which include special definitions and default actions. Special definitions are exception actions defined for a specific combination of user mode, action, and object type. These are used, for example, to hide desired attributes from the view of a certain object type. An example of a specially defined COpenAddAction action is shown in Figure 5-4. The action's page template is defined as add.thtm, the user mode as ADMIN, the object type as bridge, and the attributes to be hidden as mac and stpstate. The last element, predefined attributes, is set to empty, meaning that no attributes have been set on behalf of the user. User mode is used to implement user authorization, ensuring that each user has access only to the desired actions. For example, we don't want to allow the monitor user to add management objects, so no special or default definitions of the COpenAddAction or CAddAction class are created for the user mode MONITOR.

Figure 5-4. Special specification for the COpenAddAction function.

In addition to specific definitions, default actions with an open object type are defined for desired user modes and action types. If an action corresponding to the request is not found, an object of the CAction base class is used. The CActionDefinitions object is created in the context of the CDispatcher::Init() method, so the lifetime of the object corresponds in practice to the lifetime of the application. This provides power savings, as action objects only need to be created once during server startup. This also means that the methods of the action objects must be stateless (re-entrant) to avoid conflicts caused by the use of shared data when processing concurrent requests. Thus, the execution method of the action object does not retain information between its executions or the handling of HTTP requests, but only operates on the basis of the inputs it receives and the data it receives from the model. All state information between requests and between action executions is stored in the view, the CRequest object, and the persistent data model.

CAction

CAction is the base class for action classes, containing inherited variables and methods that are necessary for most actions. Inheritance reduces the amount of code lines required and thus the amount of programming work, because only the specific implementations and features need to be defined for inherited actions. From the perspective of application control, the most important method of the CAction class is Execute(), which requires a CRequest object as input and returns a boolean value indicating the success of the execution. The purpose of the Execute() method is to hide the error codes returned by the action-specific ExecuteAction() methods that actually perform the actions, thereby facilitating decision-making on the success of the execution in the upper layer. This interface is the same for all classes inherited from the CAction class, even though their internal implementation varies.

The CAction::Execute() method returns a boolean value indicating whether the requested action has completed. Some requests require more than one action to be performed, in which case the return value of the method can be used to determine whether a new action needs to be retrieved or whether program execution can continue. For example, after a save operation, it is not advisable to return an empty view to the user, but it is more sensible to execute the CDisplayAction of that object, which allows the object to be displayed.

COpenAddAction

COpenAddAction is responsible for creating the management object addition form. It first creates a management object instance based on the given object type and its predefined key attributes. Based on the object instance, a form is created in the view with input fields corresponding to the management object's attributes. This class does not require information from the device, but it does require the Config API interface to create the correct management object instance. The management object instance is stored in the ViewData record, which also contains other necessary information for presenting the addition form, such as information about predefined key attributes, displayed attributes, and errors that occurred in the previous addition attempt.

COpenEditAction

COpenEditAction is responsible for creating the edit form for an existing managed object. In its ExecuteAction() method, the COpenEditAction object first creates an instance of the object to be edited based on the key attributes of that object. Using this managed object instance, it retrieves the values of the managed object from the model, which are stored in the object and the ViewData record. The ViewData record contains the necessary information for presenting the form, including the attributes to be displayed and any errors that may have occurred in the previous edit attempt. The Config API's home::get_first() method is used to retrieve the data, which requires as input a managed object instance containing the key attribute values and a pointer to where the retrieved object will be stored. The key attributes of the managed object are used as a filter for the data retrieval. When all of its key attributes are defined, the model returns only one object based on the filter.

CDisplaySingleAction

The purpose of the CDisplaySingleAction class is to retrieve data for a management object from the model based on key attributes. The home::get_first() method is used for data retrieval. The object must also check the hidden attributes defined for the management object and populate the corresponding object-specific maps.

The specialty of the CDisplaySingleAction class is that it also needs to check the references of the objects being fetched to other objects. Based on these references, the links to the referenced objects required by the view are created. The references required by the action are defined in the CActionDefinitions class in connection with its specific definition.

CDisplayListAction

The CDisplayListAction class is responsible for retrieving management objects from the model based on filter attributes. The Config API's home::get_first() method is used to retrieve the first object. Subsequent objects are retrieved using the home::get_next() method, which requires the filter, the target object, and a pointer to the previously retrieved object as input. The decision to attempt to retrieve the next object is based on the error codes returned by the retrieval methods. The search continues if the method returns the error code dc_error_t::OK and stops when it is dc_error_t::NO_MORE_OBJECTS. As with the CDisplaySingleAction class, the listing must also ensure that hidden attributes are set to match any special specifications.

CAddAction

The CAddAction class is responsible for adding a management object with given attribute values. The CAddAction object, in its ExecuteAction() method, uses the Config API's home::add() method, which requires the object to be saved as input. Before saving, the object must verify the user's saving permission for the attributes to be set. In addition, any pre-defined attribute values for the object to be saved are set.

CUpdateAction

The CUpdateAction class resembles the CAddAction class in functionality, but the home::update() method is now used for saving, which is a method for updating an existing management object. If the object to be updated does not exist, the call returns an error code. This situation can occur, for example, when a user has opened the editing view of a management object, but another user deletes the same object before the first user has a chance to save their changes.

CDeleteAction

CDeleteAction is the simplest action class inherited from the CAction class, and its purpose is to delete the management object pointed to by key attributes. This is done using the Config API's home::delete() method, which requires the object to be deleted as input.

CSymbolTable

The CSymbolTable class inherits from the HttpdSymbolTable class provided by Seminole and is responsible for interpreting the information given to it in a format understood by page templates. It contains the necessary logic for performing various loops, comparisons, and evaluations based on commands received from the page template and ViewData records received from the controller. Figure 5-5 presents the CSymbolTable::HandleCond method, which is responsible for comparing if commands received from the page template.

For example, if a view wanted to know whether its current control attribute is a key, it could be determined with the page template command %{if:key}%. In that case, the command name "key" obtained by the HttpdConditionalCommand::Name() method on line 3 returns a boolean value to the variable mCurKey.

Figure 5-5. The CSymbolTable class's conditional handler.

The object responsible for interpreting the Seminole page template calls the conditional handler of the symbol table set for the request, i.e., the CSymbolTable::HandleCond method. The method retrieves the name of the received command and returns the result of a selected variable comparison based on it. For example, the page template command %{if:key}% returns true or false, depending on the boolean variable mCurKey's truth value.

5.2.3 Integration into the target environment

Integrating the process module into the target environment requires implementing the mgr_module_base class, defined as the base class for the modules, in a way that meets the module's characteristics. The module inherits its own module class from the base class, which it uses to locate target environment resources, such as the Config API and, through it, for example, the home interfaces of management objects. The complexity of the module class depends on how many file descriptors the module has that require I/O operations and thus communication with the rest of the system. File descriptors include files, sockets, and hardware handles for which blocking I/O calls can be made. Some Unix I/O system calls were presented in section 2.2.

Integrating the web management application into the target system was easy because no file descriptors communicating with the system were implemented for it. Therefore, to integrate the application, it was sufficient to retrieve the Config API interface in the module class, which would allow fetching the home interfaces of the management objects requested for configuration via HTTP requests. Figure 5-6 presents the web management application's main() method, the purpose of which is to create and initialize a module class object. On line 3, a server object is created that contains the necessary methods for receiving HTTP requests and sending responses. The dc_module_web::init() method, executed on line 7, performs the calling of the base class's init() method and the retrieval of the Config API interface. After this, on line 8, the created server object is initialized, which means initializing the embedded file system and setting HTTP request handlers for the server object. The implemented web management application used a single request handler, whose task was to parse the request parameters and pass them to the lower-level application logic. Finally, the server is started with the Httpd::Start() method, causing it to remain in an infinite loop receiving and processing requests arriving at the HTTP port.

Figures 5-6. The main() method of the web management application.

To achieve broader integration, the management application could use the dc_module_base::run() method, which registers the module with the system's internal monitoring module. This would allow, for example, the use of IPC messages to modify the management application's settings at runtime. However, for the first version of the management application, implementing basic functionality was sufficient.

5.2.4 Creating the response

Objects created from the classes presented in the previous subsection perform the most important task of a web server, which is to form an HTTP response based on the received request. The execution flow is multi-stage, and the associated objects are formed from approximately ten classes, if only the classes essential to the implemented web application are counted. A typical simplified event sequence diagram of the relationships between objects is shown in Figure 5-7. In addition to the objects presented there, the execution involves numerous objects of Seminole's internal classes, which are responsible for the lower-level operation of the server and tasks related to the processing of page templates. However, Seminole's own classes are not discussed, with a few exceptions.

Figures 5-7. Sequence diagram of HTTP request handling.

When an HTTP request arrives at the server with a prefix directed to the CWebHandler class, the request is routed to an object created from it, and more specifically to its Handle() method. The Handle() method goes through the request processing steps specific to the handler class, which for the HttpdFileHandler handler class we use, are the TranslateUri() and ProcessUri() methods in the Handle() method. In the implemented web application, the handler implemented the TranslateUri() method and the SendFile() method, which is called through the ProcessUri() method. The task of the first method is to parse the parameters of the received HTTP request into a format that the web application understands. The latter is responsible for sending the file defined in the previous steps to the user, and we will return to it at the end of the subsection. Figure 5-7 shows only the Handle() method according to the class's public interface.

The portion described above corresponds to the server-side part of the web software, which in this case is implemented in a way that corresponds to the API interface of the Seminole server. The most important part of the web software takes place in its control, where negotiation with the model occurs using the action object assigned to the requested function. In practice, the execution of the request is directed to the CControl object defined for the CWebHandler object, whose ProcessRequest() method the handler calls. The handler provides the ProcessRequest() method with the parsed action, the object type name, and the rest of the request, which it stores in a request-specific CRequest object. For efficiency reasons, only pointers are passed for the last two instead of the entire object. After the method's execution, the CRequest object should contain the page template and symbol table needed to create the view. The method's operation is shown in Figure 5-8.

Figures 5-8. CControl::ProcessContent() method.

The ProcessContent() method initially creates an empty CAction pointer. In addition, it creates a boolean variable for error checking and initializes it to false, indicating that the action has not yet been executed. Next, execution proceeds to a while loop, where the required action object pointer is retrieved from the CDispatcher object into the acn variable on line 6, and its Execute() method is called on line 7. The action object to be retrieved is determined based on the parameters of the CRequest object. The loop's task is to examine the error-indicating boolean value 'done' returned by the Execute() method, and if an error or a new action definition occurs, retrieve a new action. The Execute() method hides the actual action-executing ExecuteAction() method and its more versatile error codes. Evaluating the actual error codes is especially important, for example, when saving form inputs, because it must be possible to return to the form view if the user's input is incorrect. In addition, if no object to display is found for the show view, another view can be displayed instead, such as the object's add form.

When the execution of the request is completed, the Execute() method of the action object returns true, allowing an exit from the while loop. After that, the ProcessSymbols() method of the CRequest object is called on line 10, taking the state defined for the navigation menu as input. Using this method, the CRequest object creates a CSymbolTable object for itself, giving it the object type name, the state of the navigation menu, and the data received from the action. The task of the CSymbolTable object is to convert the data received from the model into a format understood by the page templates, based on various error and visibility information, as well as object attributes. After this, the request processing is completed from the control's perspective, and an int value indicating the success of the processing can be returned to the request handler.

The CWebHandler object that made the processing call continues execution and saves the filename of the page template obtained based on the request to its designated variable. The last step performed by the object is its SendFile() method, which executes the sending of the file specified for the handler to the user. Since the file in this case is an HTML page template containing Seminole view calls, the static HttpdFSTemplateShell::Execute() method is called, which processes the page template defined for the handler and generates a unique view based on the data pointed to by the HttpdSymbolTable object.

6. Outcome

For the purposes of this master's thesis, the administrative application was considered sufficiently complete when the functionality of the web administrative application largely matched the functionality of the CLI administrative application in scope. This required that most of the page templates required by the views and the features required by the application engine had been implemented, even if there were still shortcomings in their utilization. This chapter examines the results achieved at this level of completion.

6.1 Suitability for intended use

The goal of the web-based management application is to provide the user with an easy-to-use, browser-based management interface that matches the versatility of the existing CLI management application as comprehensively as possible. From these perspectives, the application's implementation was largely successful, with the exception of a few currently missing features. The graphical user interface offers the user an easily learned, memorable, consistent, and clear way to manage the device, and it is particularly suitable for users who only occasionally modify device settings. On the other hand, the CLI management application remains in use, so advanced users can still take advantage of efficient command-line commands.

The management application ensures the network endpoint's required security through user authentication and optional SSL encryption. Each user with login privileges is assigned a permission level from three options: ADMIN, MONITOR, and ENGINEER. The monitoring and management features visible to a user depend on their level; for example, a monitor user can only view device settings. SSL encryption is not enabled by default but can be easily enabled during server startup if remote management or an otherwise untrusted environment requires ensuring data confidentiality. Currently, SSL encryption can only be enabled through compile-time settings, but enabling it via startup parameters can be implemented if needed.

Authorization of functions is based on the user identification provided by authentication and the permission levels assigned to the function. Authorization is based on the permission level assigned to the function, based on which the use of the function is directed to the user who has the access right. For example, in the function defined in picture 5-4 on page 65, the permission level is ADMIN, in which case the function can only be used with the device administrator's credentials. Authorization allows you to easily define different views for different permission levels of a function by setting a separate page template for each permission level.

After logging into the administration application, the user lands on a homepage with a navigation menu on the left and device information displayed on the page. With a few exceptions, the buttons in the navigation menu take the user to the listing view of the desired management object, an example of which is the bridge listing view in Figure 6-1. The listing view includes buttons to display and delete existing bridges, and to add a new bridge. In addition to the management attributes displayed in the list, each bridge also contains other attributes that the user can view in the show view of the desired bridge, which is presented in Appendix 16. However, the user cannot see all attributes through the administration application; some are set to be completely hidden in connection with special configuration of functions. The edit button in the show view allows the user to edit the bridge in its edit view, which is presented in Appendix 17. The attributes set to be editable for the bridge are different from those set to be displayed, so, for example, its MAC address is not editable by the user, but is determined according to the device address set for the device.

Figure 6-1. Device bridges listing view.

The consistency and smoothness of transitions between views are control object-specific, so different view types work better for some objects and worse for others. On average, however, the views are clear and intuitive, allowing for quick navigation and finding the functionalities users need. Once the basic functionality of the management application is in place, its usability and the versatility of the views can be improved by creating new view types that better take into account the characteristics of different management objects.

The performance and memory requirements of the management application also appeared good based on initial tests. The speed of the web application was evaluated by measuring the time it took to transmit a request between the HTTP interface and the device's Config Manager. Measurements showed that the time consumed by the web application to process requests was only a fraction of the time used by the Config Manager, so it would in no way become a bottleneck in request processing. Appendix 18 presents the time taken to build a listing view containing a single bridge. The results show that the Config Manager used approximately 1.5 seconds for data retrieval using IPC messages, while the web application itself consumed about 0.6 seconds. Most of this time is spent processing management objects using Config API calls, the execution speed of which the web application cannot influence. If the time spent between the first and last Config API call is subtracted from the total execution time, the time consumed by the web application is only a few tens of milliseconds. The measurement results should be evaluated relatively, as the device used had a clock frequency of only 200MHz instead of the 300MHz clock frequency of the final target environment. The application occupies approximately 800 kilobytes of the device's Flash memory in total, of which 124 kilobytes consist of image files for HTML buttons. The application uses approximately 12.5 megabytes of RAM, most of which consists of the dynamically linked libraries it loads.

6.2 Further Development

Once the management application for the Iris 440 SHDSL device is finalized, the next step is to adapt it for the Iris 800 DSLAM device. The Iris 800 is a central office x86-based DSLAM device running on a Linux operating system, which is more powerful and has more extensive interfaces than the customer-premises Iris 440 device. From the management application's perspective, the devices are very similar, as the Iris 800 device's application environment is also based on modules as described in section 3.2, and most modules function in the same way as in this project's target environment. The biggest difference between the devices is the number of interfaces, as the Iris 440 device contains four SHDSL lines connected as a single port, while the Iris 800 device can contain a maximum of 32. In addition, the current target environment has four Ethernet interfaces, while the Iris 800 device has only one. Ultimately, the application's functionality must also be carefully tested in both environments. The use cases utilized in the design of the user interface can be used as the basis for testing.

Migrating the management application to a similar device should be easy because all of its functions are customizable through configurations, so, for example, changing the number of manageable SHDSL lines from the current one to 32 only requires reconfiguring the functions that utilize the number of lines. If the format of the management objects and the interfaces used to handle them remain unchanged, the web application code does not need to be touched. The methods of the function classes operate entirely based on the given parameters, received HTTP requests, and data received from the device, in relation to the data content and quantity of the management objects.

From the outset, the design and implementation of the management application aimed for a solution as platform-independent as possible, so that transferring the software from one device to another should not be a problem if the device management interface remains the same. Although the final application includes object type-specific functions and views, management objects can also be handled with generic default functions and views without special definitions if necessary. This means that the software is not tied to specific object types or their limitations, such as the number of possible SHDSL or Ethernet interfaces. In theory, the software can be transferred to a new product project as is and use only the default functions, and add special definitions for object type functions and related page templates only later. For security reasons, the functions for modifying management objects are not enabled by default, so only viewing objects is possible with the default functions.

Additionally, the web management application may also be transferred to a completely different device platform or operating system in the future. The Seminole server application used by the management application supports portability through its modularity and, in particular, the portability layer it offers. The portability layer refers to a part of the program isolated from the rest of the program, which contains all the application platform-specific code. Compiling this code utilizes application platform-specific configuration files, which allow the program to be easily converted to be compatible with the desired platform. In addition to Unix, Seminole includes ready-made support for MacOSX, Windows, and eCos systems, among others.

7 Summary

This master's thesis examined the different stages of designing and implementing a web management application for an embedded system. The resulting management application included a third-party HTTP server application and a self-implemented web application. For the work, the key technologies of web environments were studied in terms of server operation and security, as well as the implementation of a web application. Particularly in-depth familiarization was required with the application environment of the Iris 440 device, which served as the target environment, and the programming interfaces of Seminole, which was used as the HTTP server. The web management application was created alongside an already implemented CLI management application to offer the Iris 440 device administrator the possibility to view and manage the device's settings using a web browser. The graphical management application would provide a user-friendly alternative to the terminal interface, which many find off-putting. Despite the user-friendliness, the versatility of the management possibilities was not to be compromised; the starting point was that at least everything that could be done with the terminal should be possible with the browser. The only shortcoming compared to the CLI management application was the importing or exporting of ready-made configuration settings from the application. This was left out of the design at this stage due to technical differences.

The user interface design was primarily done by examining the features of the CLI management application and transforming the commands and management modes it offered to be suitable for a web user interface. Thus, for example, the CLI's bridge configuration mode can be equated to the bridges tab of the web application, and the interface configuration mode to the interfaces tab. Implementing the HTML page templates corresponding to the user interface view types was easy because the use of page template commands was similar to the scripting languages commonly used in web design. The clearly most difficult part of the work was the implementation of the control portion of the web application. In the early stages of implementation, various prototypes were created, based on which the software was started to be implemented in what seemed like the easiest direction. The resulting application engine ultimately met the requirements set for it in terms of generality and extensibility reasonably well. Different functions are easily definable and the page templates used are separate from the application logic, so changes to management objects or views do not require changes to the application logic. Function specifications were discussed on page 65 and Seminole server page templates on page 49.

The goal was to minimize repetition in definitions and views, allowing necessary changes to be implemented with as little effort as possible. Although the use of existing application frameworks was considered, the implementation ultimately involved building the web application from scratch, except for the templating engine used to create the views. The class architecture and internal operations of the web application were created independently, with the exception of a few classes provided by Seminole. The programming language used was C++, which is well-suited for embedded systems in terms of speed, although it is not necessarily the most straightforward language for implementing a web application. Problems arose, among other things, with the data structures used by the application, many of which were rewritten as the project progressed. Programming was facilitated by, among other things, the ready-made data structures offered by the STL library. A higher-level language such as Java and its application frameworks might have been chosen if it had been possible to use a Java virtual machine in the target environment.

References

[ASF05], The Apache Software Foundation (2005), Apache API Notes,
http://httpd.apache.org/docs/1.3/misc/API.html. (Retrieved 18.3.2007).
[AT01], Aprelium Technologies (2001), Abyss Web Server,
http://www.aprelium.com/abyssws. (Retrieved 19.3.2007).
[Ber99], Berners-Lee, T. et al. (1999), Hypertext Transfer Protocol - HTTP/1.1. IETF
RFC2616, http://www.ietf.org/rfc/rfc2616.txt. (Retrieved 7.11.2006).
[CN01], CollabNet, Inc. (2001), Subversion, http://subversion.tigris.org. (Retrieved
19.3.2007).
[CoRo99], Coar, K., Robinson, D., (1999), The WWW Common Gateway Interface 1.1
Revision 3. http://cgi-spec.golux.com/draft-coar-cgi-v11-03.txt. (Retrieved 17.2.2007).
[DC06], Design Combus Oy (2006), Iris 24 HW-arkkitehtuuri.
[DiAl99], Dierks, T., Allen, C., (1999), The TLS Protocol, version 1.0. IETF RFC2246,
http://www.ietf.org/rfc/rfc2246.txt (Retrieved 18.11.2006).
[Dru99], Druschel, Peter et al. (1999), Flash: An Efficient and Portable Web Server.
USENIX Annual Technical Conference Monterey, California, USA, June 6-11, 1999.
http://www.usenix.org/events/usenix99/full_papers/pai/pai.pdf. (Retrieved 6.1.2007).
[Fra99], Franks, J. et al. (1999), Hypertext Transfer Protocol - HTTP Authentication:
Basic and Digest Access Authentication. IETF RFC2617,
http://www.ietf.org/rfc/rfc2617.txt. (Retrieved 19.11.2006).
[Fre06], Freier, Alan et al. (1996), SSL 3.0 Specification. Netscape Internet Draft,
http://wp.netscape.com/eng/ssl3/. (Retrieved 18.11.2006).
[FSF91], Free Software Foundation, Inc. (1991), GNU General Public License.
http://www.gnu.org/licenses/gpl.txt. (Retrieved 19.2.2007).
[FSF97], Free Software Foundation, Inc. (1997), GNU Make.
http://www.gnu.org/software/make. (Retrieved 19.3.2007).
[GA00], GoAhead Software Inc. (2000), GoAhead WebServer 2.1 datasheet,
http://data.goahead.com/webserver/WS-Datasheet5-00.cra.pdf. (Retrieved 7.1.2007).
[Gru86], Grune, Dick (1986), Concurrent Versions System, http://www.nongnu.org/cvs.
(Retrieved 19.3.2007).
[GS06a], GladeSoft, Inc. (2006), Seminole Datasheet,
http://www.gladesoft.com/products/seminole/semdatasheet.pdf. (Retrieved 20.11.2006).
[GS06b], GladeSoft, Inc. (2006), Seminole Developer’s Guide,
http://www.gladesoft.com/products/seminole/semman.pdf. (Retrieved 20.11.2006).
[HaJä03], Haikala, Ilkka, Järvinen, Hannu-Matti (2003), Käyttöjärjestelmät. Helsinki:
Talentum. pp. 242
[HaMä98], Haikala, Ilkka, Märijärvi, Jukka (1998), Ohjelmistotuotanto, 6. painos.
Helsinki: Suomen ATK-kustannus Oy. pp. 389
[HuTh00], Hunt, Andrew, Thomas, David (2000), The Pragmatic Programmer. Boston:
Addison-Wesley. pp. 321
[Kal95], Kalimo, Anna (1995), Graafisen käyttöliittymän suunnittelu. Helsinki: Tieke
RY. pp. 229
[Ker02a], Keren, Guy (2002), Unix Multi-Process Programming and Inter-Process
Communications (IPC), http://users.actcom.co.il/~choo/lupg/tutorials/multiprocess/multi-process.html. (Retrieved 26.11.2006).
[Ker02b], Keren, Guy (2002), Network programming under Unix systems,
http://users.actcom.co.il/~choo/lupg/tutorials/internetworking/internetprogramming.html#sockets. (Retrieved 26.11.2006).
[Ker98], Kerttula, Matti (1998), Tietoverkkojen tietoturva. Helsinki: Oy Edita Ab. pp. 510
[KL07], KoanLogic (2007), Koanlogic Klone tutorial,
http://www.koanlogic.com/kl/cont/gb/html/klone-tute.html. (Retrieved 14.1.2007).
[Kuu03], Kuutti, Wille (2003), Käytettävyys, suunnittelu ja arviointi. Helsinki:
Talentum. pp. 191
[Maj05], Maj, Arthur (2005), Apache 2 with SSL/TLS: Step-by-Step,
http://www.securityfocus.com/infocus/1818. (Retrieved 18.2.2007).
[Mas05], Mason, Mike (2005), Pragmatic Version Control using Subversion. Dallas:
The Pragmatic Programmers. pp. 207
[Mbe03], Mbedthis Software LLC (2003), AppWeb/Mbedthis,
http://www.mbedthis.com. (Retrieved 19.3.2007).
[MS95], Microsoft Corporation (1995), Internet Server API, http://msdn.microsoft.com.
(Retrieved 19.3.2007).
[Mäk04], Mäkitalo, Tommi (2004), Tntnet Web Server, http://www.tntnet.org. (Retrieved
19.3.2007).
[OG01], The Open Group (2001), The Single Unix Specification, Version 3,
http://www.opengroup.org/unix/online.html. (Retrieved 19.3.2007).
[OM96], Open Market, Inc. (1996), FastCGI: A High-Performance Web Server
Interface, http://www.fastcgi.com/devkit/doc/fastcgi-whitepaper/fastcgi.htm. (Retrieved
18.3.2007).
[OvRa98], Ovaska, Saila, Räihä, Kari-Jouko (1998), Käytettävyystestaus. Sytyke RY,
Systeemityö 4/98. pp. 13-14.
[Pel98], Peltomäki, Juha (1998), WWW-ohjelmointi. Jyväskylä: Teknolit. pp. 722
[Per96], Perens, Bruce (1996), BusyBox, http://www.busybox.net. (Retrieved 18.3.2007).
[Pus01], Puska, Matti (2001), Linux palvelimena. Helsinki: Satku. pp. 304
[RuCo01], Cobert, Jonathan, Rubini, Alessandro (2001), Linux Device Drivers: Second
Edition. Sebastopol, CA: O'Reilly & Associates Inc., pp. 564
[Yag03], Yaghmour, Karim (2003), Building Embedded Linux Systems. Sebastopol,
CA: O'Reilly & Associates, Inc., pp. 391
[ZT95], Zeus Technologies Ltd. (1995), Zeus Web Server,
http://www.zeus.com/products/zws. (Retrieved 19.3.2007).

Appendix

Appendix 1. Subversion commands with explanations (synonyms in parentheses).
add - save an item to version control.
blame (praise, annotate, ann) - print the content of a file or URL with author annotations.
cat - print the content of a file or URL.
checkout (co) - retrieve an item to a local directory.
cleanup - clean up the working copy, including removing locks and uncommitted changes.
commit (ci) - save changes from the working copy to version control.
copy (cp) - make a copy of an item to the desired location.
delete (del, remove, rm) - delete an item.
diff (di) - show the difference between two items.
export - create an unversioned copy of the head revision.
help (?, h) - show help.
import - save changes from an unversioned copy to version control.
info - show information about an item.
list (ls) - print the contents of a directory.
lock - lock the desired item, preventing other users from saving changes to it.
log - show explanations of changes to the desired item.
merge - merge the desired items.
mkdir - create a new directory.
move (mv, rename, ren) - move an item to the desired location.
propdel (pdel, pd) - delete item property data.
propedit (pedit, pe) - edit item property data.
propget (pget, pg) - retrieve the desired item property data.
proplist (plist, pl) - retrieve all item property data.
propset (pset, ps) - set the desired property data for an item.
resolved - remove conflict markers from an item.
revert - undo changes made to an item.
status (stat, st) - show the status of an item in the working copy.
switch (sw) - save the working copy to a new storage location.
unlock - unlock the desired working copy or URL.
update (up) - update the working copy with changes from version control.

Appendix 2. HTTP servers included in the mapping.

Appendix 3. User interface design of the bridge editing view.

Appendix 4. User interface design plan bridge addition view.

Appendix 5. User Interface Design - Bridge Listing View.

Appendix 6. Ethernet interface listing view of the user interface design.

Appendix 7. View of the user interface design for listing package counters.

Appendix 8. Disk image loading view of the user interface design.

Appendix 9. Installation view of the user interface design disk image.

Appendix 10. View of the user interface design for an incomplete disk image installation.

Appendix 11. Device configuration menu of the user interface design.

Appendix 12. View of the user interface design for the request confirmation query.

Appendix 13. View of the user interface design upon completion of disk image installation.

Appendix 14. User interface design front page view.

Appendix 15. Displaying the SNMP community of the user interface design.

Appendix 16. Displaying the bridge in the final web management application.

Appendix 17. Modifying the bridge in the final web management application.

Appendix 18. Time spent listing a single management object.

Measurement 1:
00:05:56,939 [1024] TRACE <> - timing: starting process
00:05:56,989 [1024] TRACE <> - class obj * obj::clone() const
00:05:56,989 [1024] TRACE <> - class dc_error_t obj::serialize() const
00:05:56,989 [1024] TRACE <> - static class dc_error_t
obj::deserializer()
00:05:56,999 [1024] TRACE <> - class dc_error_t
remote_home::get_first() const
00:05:57,019 [1024] TRACE <> - <--- send msg
00:05:57,869 [1024] DEBUG <> - >--- recv msg
00:05:58,159 [1024] TRACE <> - class obj * obj::clone() const
00:05:58,159 [1024] TRACE <> - class dc_error_t obj::serialize() const
00:05:58,189 [1024] TRACE <> - static class dc_error_t
obj::deserializer()
00:05:58,209 [1024] TRACE <> - class dc_error_t
remote_home::get_next() const
00:05:58,269 [1024] TRACE <> - <--- send msg
00:05:58,899 [1024] DEBUG <> - >--- recv msg
00:05:58,959 [1024] TRACE <> - class dc_error_t obj::serialize() const
00:05:58,969 [1024] TRACE <> - static class dc_error_t
obj::deserializer()
00:05:59,069 [1024] TRACE <> - timing: process done
IPC delay: 1,48s
total: 2,13s

Measurement 2:
00:09:42,499 [1024] TRACE <> - timing: starting process
00:09:42,499 [1024] TRACE <> - class obj * obj::clone() const
00:09:42,499 [1024] TRACE <> - class dc_error_t obj::serialize() const
00:09:42,509 [1024] TRACE <> - static class dc_error_t
obj::deserializer()
00:09:42,509 [1024] TRACE <> - class dc_error_t
remote_home::get_first() const
00:09:42,529 [1024] TRACE <> - <--- send msg
00:09:43,379 [1024] DEBUG <> - >--- recv msg
00:09:43,659 [1024] TRACE <> - class obj * obj::clone() const
00:09:43,669 [1024] TRACE <> - class dc_error_t obj::serialize() const
00:09:43,699 [1024] TRACE <> - static class dc_error_t
obj::deserializer()
00:09:43,709 [1024] TRACE <> - class dc_error_t
remote_home::get_next() const
00:09:43,769 [1024] TRACE <> - <--- send msg
00:09:44,409 [1024] DEBUG <> - >--- recv msg
00:09:44,469 [1024] TRACE <> - class dc_error_t obj::serialize() const
00:09:44,469 [1024] TRACE <> - static class dc_error_t
obj::deserializer()
00:09:44,569 [1024] TRACE <> - timing: process done
IPC delay: 1,49s
total: 2,07s
